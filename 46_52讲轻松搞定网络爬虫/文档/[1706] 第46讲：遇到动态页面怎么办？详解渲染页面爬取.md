<p data-nodeid="165308">前面我们已经介绍了 Scrapy 的一些常见用法，包括服务端渲染页面的抓取和 API 的抓取，Scrapy 发起 Request 之后，返回的 Response 里面就包含了想要的结果。</p>



<p data-nodeid="163554">但是现在越来越多的网页都已经演变为 SPA 页面，其页面在浏览器中呈现的结果是经过 JavaScript 渲染得到的，如果我们使用 Scrapy 直接对其进行抓取的话，其结果和使用 requests 没有什么区别。</p>
<p data-nodeid="163555">那我们真的要使用 Scrapy 完成对 JavaScript 渲染页面的抓取应该怎么办呢？</p>
<p data-nodeid="163556">之前我们介绍了 Selenium 和 Pyppeteer 都可以实现 JavaScript 渲染页面的抓取，那用了 Scrapy 之后应该这么办呢？Scrapy 能和 Selenium 或 Pyppeteer 一起使用吗？答案是肯定的，我们可以将 Selenium 或 Pyppeteer 通过 Downloader Middleware 和 Scrapy 融合起来，实现 JavaScript 渲染页面的抓取，本节我们就来了解下它的实现吧。</p>
<h3 data-nodeid="165800" class="">回顾</h3>

<p data-nodeid="163558">在前面我们介绍了 Downloader Middleware 的用法，在 Downloader Middleware 中有三个我们可以实现的方法 process_request、process_response 以及 process_exception 方法。</p>
<p data-nodeid="163559">我们再看下 process_request 方法和其不同的返回值的效果：</p>
<ul data-nodeid="163560">
<li data-nodeid="163561">
<p data-nodeid="163562">当返回为 None 时，Scrapy 将继续处理该 Request，接着执行其他 Downloader Middleware 的 process_request 方法，一直到 Downloader 把 Request 执行完后得到 Response 才结束。这个过程其实就是修改 Request 的过程，不同的 Downloader Middleware 按照设置的优先级顺序依次对 Request 进行修改，最后送至 Downloader 执行。</p>
</li>
<li data-nodeid="163563">
<p data-nodeid="163564">当返回为 Response 对象时，更低优先级的 Downloader Middleware 的 process_request 和 process_exception 方法就不会被继续调用，每个 Downloader Middleware 的 process_response 方法转而被依次调用。调用完毕之后，直接将 Response 对象发送给 Spider 来处理。</p>
</li>
<li data-nodeid="163565">
<p data-nodeid="163566">当返回为 Request 对象时，更低优先级的 Downloader Middleware 的 process_request 方法会停止执行。这个 Request 会重新放到调度队列里，其实它就是一个全新的 Request，等待被调度。如果被 Scheduler 调度了，那么所有的 Downloader Middleware 的 process_request 方法都会被重新按照顺序执行。</p>
</li>
<li data-nodeid="163567">
<p data-nodeid="163568">如果 IgnoreRequest 异常抛出，则所有的 Downloader Middleware 的 process_exception 方法会依次执行。如果没有一个方法处理这个异常，那么 Request 的 errorback 方法就会回调。如果该异常还没有被处理，那么它便会被忽略。</p>
</li>
</ul>
<p data-nodeid="163569">这里我们注意到第二个选项，当返回结果为 Response 对象时，低优先级的 process_request 方法就不会被继续调用了，这个 Response 对象会直接经由 process_response 方法处理后转交给 Spider 来解析。</p>
<p data-nodeid="163570">然后再接着想一想，process_request 接收的参数是 request，即 Request 对象，怎么会返回 Response 对象呢？原因可想而知了，这个 Request 对象不再经由 Scrapy 的 Downloader 来处理了，而是在 process_request 方法里面直接就完成了 Request 的发送操作，然后在得到了对应的 Response 结果后再将其返回就好了。</p>
<p data-nodeid="163571">那么对于 JavaScript 渲染的页面来说，照这个方法来做，我们就可以把 Selenium 或 Pyppeteer 加载页面的过程在 process_request 方法里面实现，得到网页渲染完后的源代码后直接构造 Response 返回即可，这样我们就完成了借助 Downloader Middleware 实现 Scrapy 爬取动态渲染页面的过程。</p>
<h3 data-nodeid="166292" class="">案例</h3>

<p data-nodeid="163573">本节我们就用实例来讲解一下 Scrapy 和 Pyppeteer 实现 JavaScript 渲染页面抓取的流程。</p>
<p data-nodeid="167272">本节使用的实例网站为 <a href="https://dynamic5.scrape.center/" data-nodeid="167277">https://dynamic5.scrape.center/</a>，这是一个 JavaScript 渲染页面，其内容是一本本的图书信息。</p>
<p data-nodeid="167273" class=""><img src="https://s0.lgstatic.com/i/image/M00/34/22/Ciqc1F8RXwWARdOHABEEvYwMNOY314.png" alt="image.png" data-nodeid="167281"></p>



<p data-nodeid="163576">同时这个网站的页面带有分页功能，只需要在 URL 加上 <code data-backticks="1" data-nodeid="163693">/page/</code> 和页码就可以跳转到下一页，如 <a href="https://dynamic5.scrape.center/page/2" data-nodeid="163697">https://dynamic5.scrape.center/page/2</a> 就是第二页内容，<a href="https://dynamic5.scrape.center/page/3" data-nodeid="163701">https://dynamic5.scrape.center/page/3</a> 就是第三页内容。</p>
<p data-nodeid="163577">那我们这个案例就来试着爬取前十页的图书信息吧。</p>
<h3 data-nodeid="167772" class="">实现</h3>

<p data-nodeid="163579">首先我们来新建一个项目，叫作 scrapypyppeteer，命令如下：</p>
<pre class="lang-java" data-nodeid="169737"><code data-language="java">scrapy startproject scrapypyppeteer
</code></pre>




<p data-nodeid="163581">接着进入项目，然后新建一个 Spider，名称为 book，命令如下：</p>
<pre class="lang-java" data-nodeid="170228"><code data-language="java">cd scrapypyppeteer
scrapy genspider book dynamic5.scrape.center
</code></pre>

<p data-nodeid="163583">这时候可以发现在项目的 spiders 文件夹下就出现了一个名为 spider.py 的文件，内容如下：</p>
<pre class="lang-dart" data-nodeid="176857"><code data-language="dart"># -*- coding: utf<span class="hljs-number">-8</span> -*-
<span class="hljs-keyword">import</span> scrapy
​
​
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BookSpider</span>(<span class="hljs-title">scrapy</span>.<span class="hljs-title">Spider</span>):
 &nbsp; &nbsp;<span class="hljs-title">name</span> = '<span class="hljs-title">book</span>'
 &nbsp; &nbsp;<span class="hljs-title">allowed_domains</span> = ['<span class="hljs-title">dynamic5</span>.<span class="hljs-title">scrape</span>.<span class="hljs-title">center</span>']
 &nbsp; &nbsp;<span class="hljs-title">start_urls</span> = ['<span class="hljs-title">http</span>://<span class="hljs-title">dynamic5</span>.<span class="hljs-title">scrape</span>.<span class="hljs-title">center</span>/']
​
 &nbsp; &nbsp;<span class="hljs-title">def</span> <span class="hljs-title">parse</span>(<span class="hljs-title">self</span>, <span class="hljs-title">response</span>):
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-title">pass</span>
</span></code></pre>














<p data-nodeid="163585">首先我们构造列表页的初始请求，实现一个 start_requests 方法，如下所示：</p>
<pre class="lang-go" data-nodeid="191096"><code data-language="go"># -*- coding: utf<span class="hljs-number">-8</span> -*-
from scrapy <span class="hljs-keyword">import</span> Request, Spider
​
​
class BookSpider(Spider):
 &nbsp; &nbsp;name = <span class="hljs-string">'book'</span>
 &nbsp; &nbsp;allowed_domains = [<span class="hljs-string">'dynamic5.scrape.center'</span>]
 &nbsp; &nbsp;
 &nbsp; &nbsp;base_url = <span class="hljs-string">'https://dynamic5.scrape.center/page/{page}'</span>
 &nbsp; &nbsp;max_page = <span class="hljs-number">10</span>
 &nbsp; &nbsp;
 &nbsp; &nbsp;def start_requests(self):
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">for</span> page in <span class="hljs-keyword">range</span>(<span class="hljs-number">1</span>, self.max_page + <span class="hljs-number">1</span>):
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;url = self.base_url.format(page=page)
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;yield Request(url, callback=self.parse_index)
 &nbsp; &nbsp;
 &nbsp; &nbsp;def parse_index(self, response):
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-built_in">print</span>(response.text)
</code></pre>





























<p data-nodeid="192069">这时如果我们直接运行这个 Spider，在 parse_index 方法里面打印输出 Response 的内容，结果如下：</p>
<p data-nodeid="192070" class=""><img src="https://s0.lgstatic.com/i/image/M00/34/2D/CgqCHl8RX0iADQnIAAJk6NIEDak825.png" alt="image (1).png" data-nodeid="192080"></p>


<p data-nodeid="163589">我们可以发现所得到的内容并不是页面渲染后的真正 HTML 代码。此时如果我们想要获取 HTML 渲染结果的话就得使用 Downloader Middleware 实现了。</p>
<p data-nodeid="163590">这里我们直接以一个我已经写好的组件来演示了，组件的名称叫作 GerapyPyppeteer，组件里已经写好了 Scrapy 和 Pyppeteer 结合的中间件，下面我们来详细介绍下。</p>
<p data-nodeid="163591">我们可以借助于 pip3 来安装组件，命令如下：</p>
<pre class="lang-java" data-nodeid="192579"><code data-language="java">pip3 install gerapy-pyppeteer
</code></pre>

<p data-nodeid="163593">GerapyPyppeteer 提供了两部分内容，一部分是 Downloader Middleware，一部分是 Request。<br>
首先我们需要开启中间件，在 settings 里面开启 PyppeteerMiddleware，配置如下：</p>
<pre class="lang-java" data-nodeid="193078"><code data-language="java">DOWNLOADER_MIDDLEWARES = {
 &nbsp; &nbsp;<span class="hljs-string">'gerapy_pyppeteer.downloadermiddlewares.PyppeteerMiddleware'</span>: <span class="hljs-number">543</span>,
}
</code></pre>

<p data-nodeid="163595">然后我们把上文定义的 Request 修改为 PyppeteerRequest 即可：</p>
<pre class="lang-go" data-nodeid="197569"><code data-language="go"># -*- coding: utf<span class="hljs-number">-8</span> -*-
from gerapy_pyppeteer <span class="hljs-keyword">import</span> PyppeteerRequest
from scrapy <span class="hljs-keyword">import</span> Request, Spider
​
​
class BookSpider(Spider):
 &nbsp; &nbsp;name = <span class="hljs-string">'book'</span>
 &nbsp; &nbsp;allowed_domains = [<span class="hljs-string">'dynamic5.scrape.center'</span>]
 &nbsp; &nbsp;
 &nbsp; &nbsp;base_url = <span class="hljs-string">'https://dynamic5.scrape.center/page/{page}'</span>
 &nbsp; &nbsp;max_page = <span class="hljs-number">10</span>
 &nbsp; &nbsp;
 &nbsp; &nbsp;def start_requests(self):
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">for</span> page in <span class="hljs-keyword">range</span>(<span class="hljs-number">1</span>, self.max_page + <span class="hljs-number">1</span>):
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;url = self.base_url.format(page=page)
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;yield PyppeteerRequest(url, callback=self.parse_index, wait_for=<span class="hljs-string">'.item .name'</span>)
 &nbsp; &nbsp;
 &nbsp; &nbsp;def parse_index(self, response):
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-built_in">print</span>(response.text)
</code></pre>









<p data-nodeid="163597">这样其实就完成了 Pyppeteer 的对接了，非常简单。<br>
这里 PyppeteerRequest 和原本的 Request 多提供了一个参数，就是 wait_for，通过这个参数我们可以指定 Pyppeteer 需要等待特定的内容加载出来才算结束，然后才返回对应的结果。</p>
<p data-nodeid="163598">为了方便观察效果，我们把并发限制修改得小一点，然后把 Pyppeteer 的 Headless 模式设置为 False：</p>
<pre class="lang-java" data-nodeid="198068"><code data-language="java">CONCURRENT_REQUESTS = <span class="hljs-number">3</span>
GERAPY_PYPPETEER_HEADLESS = False
</code></pre>

<p data-nodeid="199057">这时我们重新运行 Spider，就可以看到在爬取的过程中，Pyppeteer 对应的 Chromium 浏览器就弹出来了，并逐个加载对应的页面内容，加载完成之后浏览器关闭。<br>
另外观察下控制台，我们发现对应的结果也就被提取出来了，如图所示：</p>
<p data-nodeid="199058" class=""><img src="https://s0.lgstatic.com/i/image/M00/34/2D/CgqCHl8RX2SAAVD5AAK2ktEbgAQ066.png" alt="image (2).png" data-nodeid="199068"></p>


<p data-nodeid="163602">这时候我们再重新修改下 parse_index 方法，提取对应的每本书的名称和作者即可：</p>
<pre class="lang-java" data-nodeid="201096"><code data-language="java"><span class="hljs-function">def <span class="hljs-title">parse_index</span><span class="hljs-params">(self, response)</span>:
 &nbsp; &nbsp;<span class="hljs-keyword">for</span> item in response.<span class="hljs-title">css</span><span class="hljs-params">(<span class="hljs-string">'.item'</span>)</span>:
 &nbsp; &nbsp; &nbsp; &nbsp;name </span>= item.css(<span class="hljs-string">'.name::text'</span>).extract_first()
 &nbsp; &nbsp; &nbsp; &nbsp;authors = item.css(<span class="hljs-string">'.authors::text'</span>).extract_first()
 &nbsp; &nbsp; &nbsp; &nbsp;name = name.strip() <span class="hljs-keyword">if</span> name <span class="hljs-keyword">else</span> None
 &nbsp; &nbsp; &nbsp; &nbsp;authors = authors.strip() <span class="hljs-keyword">if</span> authors <span class="hljs-keyword">else</span> None
 &nbsp; &nbsp; &nbsp; &nbsp;yield {
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string">'name'</span>: name,
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string">'authors'</span>: authors
 &nbsp; &nbsp; &nbsp;  }
</code></pre>




<p data-nodeid="202354">重新运行，即可发现对应的名称和作者就被提取出来了，运行结果如下：</p>
<p data-nodeid="202355" class=""><img src="https://s0.lgstatic.com/i/image/M00/34/2E/CgqCHl8RX4OAK3y6AAOIS7PLohc610.png" alt="image (3).png" data-nodeid="202363"></p>

<p data-nodeid="202102">这样我们就借助 GerapyPyppeteer 完成了 JavaScript 渲染页面的爬取。</p>



<h4 data-nodeid="202878" class="">原理分析</h4>

<p data-nodeid="163608">但上面仅仅是我们借助 GerapyPyppeteer 实现了 Scrapy 和 Pyppeteer 的对接，但其背后的原理是怎样的呢？</p>
<p data-nodeid="163609">我们可以详细分析它的源码，其 GitHub 地址为 <a href="https://github.com/Gerapy/GerapyPyppeteer" data-nodeid="163749">https://github.com/Gerapy/GerapyPyppeteer</a>。</p>
<p data-nodeid="163610">首先通过分析可以发现其最核心的内容就是实现了一个 PyppeteerMiddleware，这是一个 Downloader Middleware，这里最主要的就是 process_request  的实现，核心代码如下所示：</p>
<pre class="lang-java" data-nodeid="203394"><code data-language="java"><span class="hljs-function">def <span class="hljs-title">process_request</span><span class="hljs-params">(self, request, spider)</span>:
 &nbsp; &nbsp;logger.<span class="hljs-title">debug</span><span class="hljs-params">(<span class="hljs-string">'processing request %s'</span>, request)</span> &nbsp;
 &nbsp; &nbsp;return <span class="hljs-title">as_deferred</span><span class="hljs-params">(self._process_request(request, spider)</span>)
</span></code></pre>

<p data-nodeid="204424">这里其实就是调用了一个 _process_request 方法，这个方法的返回结果被 as_deferred 方法调用了。</p>
<p data-nodeid="204425">这个 as_deferred 是怎么定义的呢？代码如下：</p>

<pre class="lang-java" data-nodeid="203909"><code data-language="java"><span class="hljs-keyword">import</span> asyncio
from twisted.internet.defer <span class="hljs-keyword">import</span> Deferred
​
<span class="hljs-function">def <span class="hljs-title">as_deferred</span><span class="hljs-params">(f)</span>:
 &nbsp; &nbsp;return Deferred.<span class="hljs-title">fromFuture</span><span class="hljs-params">(asyncio.ensure_future(f)</span>)
</span></code></pre>

<p data-nodeid="204950">这个方法接收的就是一个 asyncio 库的 Future 对象，然后通过 fromFuture 方法转化成了 twisted 里面的 Deferred 对象。这是因为 Scrapy 本身的异步是借助 twisted 实现的，一个个的异步任务对应的就是一个个 Deferred 对象，而 Pyppeteer 又是基于 asyncio 的，它的异步任务是 Future 对象，所以这里我们需要借助 Deferred 的 fromFuture 方法将 Future 转为 Deferred 对象。</p>
<p data-nodeid="204951">另外为了支持这个功能，我们还需要在 Scrapy 中修改 reactor 对象，修改为 AsyncioSelectorReactor，实现如下：</p>

<pre class="lang-dart" data-nodeid="210618"><code data-language="dart"><span class="hljs-keyword">import</span> sys
from twisted.internet.asyncioreactor <span class="hljs-keyword">import</span> AsyncioSelectorReactor
<span class="hljs-keyword">import</span> twisted.internet
​
reactor = AsyncioSelectorReactor(asyncio.get_event_loop())
​
# install AsyncioSelectorReactor
twisted.internet.reactor = reactor
sys.modules[<span class="hljs-string">'twisted.internet.reactor'</span>] = reactor
</code></pre>











<p data-nodeid="163616">这段代码已经在 PyppeteerMiddleware 里面定义好了，在 Scrapy 正式开始爬取之前这段代码就会被执行，将 Scrapy 中的 reactor 修改为 AsyncioSelectorReactor，从而实现 Future 的调度。<br>
接下来我们再来看下 _process_request 方法，实现如下：</p>
<pre class="lang-dart" data-nodeid="218343"><code data-language="dart"><span class="hljs-keyword">async</span> def _process_request(self, request: PyppeteerRequest, spider):
 &nbsp; &nbsp;<span class="hljs-string">"""
 &nbsp;  use pyppeteer to process spider
 &nbsp;  :param request:
 &nbsp;  :param spider:
 &nbsp;  :return:
 &nbsp;  """</span>
 &nbsp; &nbsp;options = {
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string">'headless'</span>: self.headless,
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string">'dumpio'</span>: self.dumpio,
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string">'devtools'</span>: self.devtools,
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string">'args'</span>: [
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;f<span class="hljs-string">'--window-size={self.window_width},{self.window_height}'</span>,
 &nbsp; &nbsp; &nbsp;  ]
 &nbsp;  }
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> self.executable_path: options[<span class="hljs-string">'executable_path'</span>] = self.executable_path
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> self.disable_extensions: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--disable-extensions'</span>)
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> self.hide_scrollbars: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--hide-scrollbars'</span>)
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> self.mute_audio: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--mute-audio'</span>)
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> self.no_sandbox: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--no-sandbox'</span>)
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> self.disable_setuid_sandbox: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--disable-setuid-sandbox'</span>)
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> self.disable_gpu: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--disable-gpu'</span>)
 &nbsp; &nbsp;
 &nbsp; &nbsp;# <span class="hljs-keyword">set</span> proxy
 &nbsp; &nbsp;proxy = request.proxy
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> not proxy:
 &nbsp; &nbsp; &nbsp; &nbsp;proxy = request.meta.<span class="hljs-keyword">get</span>(<span class="hljs-string">'proxy'</span>)
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> proxy: options[<span class="hljs-string">'args'</span>].append(f<span class="hljs-string">'--proxy-server={proxy}'</span>)
 &nbsp; &nbsp;
 &nbsp; &nbsp;logger.debug(<span class="hljs-string">'set options %s'</span>, options)
 &nbsp; &nbsp;
 &nbsp; &nbsp;browser = <span class="hljs-keyword">await</span> launch(options)
 &nbsp; &nbsp;page = <span class="hljs-keyword">await</span> browser.newPage()
 &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.setViewport({<span class="hljs-string">'width'</span>: self.window_width, <span class="hljs-string">'height'</span>: self.window_height})
 &nbsp; &nbsp;
 &nbsp; &nbsp;# <span class="hljs-keyword">set</span> cookies
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> isinstance(request.cookies, dict):
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.setCookie(*[
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  {<span class="hljs-string">'name'</span>: k, <span class="hljs-string">'value'</span>: v}
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> request.cookies.items()
 &nbsp; &nbsp; &nbsp;  ])
 &nbsp; &nbsp;<span class="hljs-keyword">else</span>:
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.setCookie(request.cookies)
 &nbsp; &nbsp;
 &nbsp; &nbsp;# the headers must be <span class="hljs-keyword">set</span> using request interception
 &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.setRequestInterception(True)
 &nbsp; &nbsp;
 &nbsp; &nbsp;<span class="hljs-meta">@page</span>.<span class="hljs-keyword">on</span>(<span class="hljs-string">'request'</span>)
 &nbsp; &nbsp;<span class="hljs-keyword">async</span> def _handle_interception(pu_request):
 &nbsp; &nbsp; &nbsp; &nbsp;# handle headers
 &nbsp; &nbsp; &nbsp; &nbsp;overrides = {
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string">'headers'</span>: {
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;k.decode(): <span class="hljs-string">','</span>.join(map(lambda v: v.decode(), v))
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> request.headers.items()
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  }
 &nbsp; &nbsp; &nbsp;  }
 &nbsp; &nbsp; &nbsp; &nbsp;# handle resource types
 &nbsp; &nbsp; &nbsp; &nbsp;_ignore_resource_types = self.ignore_resource_types
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">if</span> request.ignore_resource_types <span class="hljs-keyword">is</span> not None:
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;_ignore_resource_types = request.ignore_resource_types
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">if</span> pu_request.resourceType <span class="hljs-keyword">in</span> _ignore_resource_types:
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">await</span> pu_request.abort()
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">else</span>:
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">await</span> pu_request.continue_(overrides)
 &nbsp; &nbsp;
 &nbsp; &nbsp;timeout = self.download_timeout
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> request.timeout <span class="hljs-keyword">is</span> not None:
 &nbsp; &nbsp; &nbsp; &nbsp;timeout = request.timeout
 &nbsp; &nbsp;
 &nbsp; &nbsp;logger.debug(<span class="hljs-string">'crawling %s'</span>, request.url)
 &nbsp; &nbsp;
 &nbsp; &nbsp;response = None
 &nbsp; &nbsp;<span class="hljs-keyword">try</span>:
 &nbsp; &nbsp; &nbsp; &nbsp;options = {
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string">'timeout'</span>: <span class="hljs-number">1000</span> * timeout,
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string">'waitUntil'</span>: request.wait_until
 &nbsp; &nbsp; &nbsp;  }
 &nbsp; &nbsp; &nbsp; &nbsp;logger.debug(<span class="hljs-string">'request %s with options %s'</span>, request.url, options)
 &nbsp; &nbsp; &nbsp; &nbsp;response = <span class="hljs-keyword">await</span> page.goto(
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;request.url,
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;options=options
 &nbsp; &nbsp; &nbsp;  )
 &nbsp; &nbsp;except (PageError, TimeoutError):
 &nbsp; &nbsp; &nbsp; &nbsp;logger.error(<span class="hljs-string">'error rendering url %s using pyppeteer'</span>, request.url)
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.close()
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">await</span> browser.close()
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">return</span> self._retry(request, <span class="hljs-number">504</span>, spider)
 &nbsp; &nbsp;
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> request.wait_for:
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">try</span>:
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;logger.debug(<span class="hljs-string">'waiting for %s finished'</span>, request.wait_for)
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.waitFor(request.wait_for)
 &nbsp; &nbsp; &nbsp; &nbsp;except TimeoutError:
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;logger.error(<span class="hljs-string">'error waiting for %s of %s'</span>, request.wait_for, request.url)
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.close()
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">await</span> browser.close()
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">return</span> self._retry(request, <span class="hljs-number">504</span>, spider)
 &nbsp; &nbsp;
 &nbsp; &nbsp;# evaluate script
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> request.script:
 &nbsp; &nbsp; &nbsp; &nbsp;logger.debug(<span class="hljs-string">'evaluating %s'</span>, request.script)
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.evaluate(request.script)
 &nbsp; &nbsp;
 &nbsp; &nbsp;# sleep
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> request.sleep <span class="hljs-keyword">is</span> not None:
 &nbsp; &nbsp; &nbsp; &nbsp;logger.debug(<span class="hljs-string">'sleep for %ss'</span>, request.sleep)
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">await</span> asyncio.sleep(request.sleep)
 &nbsp; &nbsp;
 &nbsp; &nbsp;content = <span class="hljs-keyword">await</span> page.content()
 &nbsp; &nbsp;body = str.encode(content)
 &nbsp; &nbsp;
 &nbsp; &nbsp;# close page and browser
 &nbsp; &nbsp;logger.debug(<span class="hljs-string">'close pyppeteer'</span>)
 &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.close()
 &nbsp; &nbsp;<span class="hljs-keyword">await</span> browser.close()
 &nbsp; &nbsp;
 &nbsp; &nbsp;<span class="hljs-keyword">if</span> not response:
 &nbsp; &nbsp; &nbsp; &nbsp;logger.error(<span class="hljs-string">'get null response by pyppeteer of url %s'</span>, request.url)
 &nbsp; &nbsp;
 &nbsp; &nbsp;# Necessary to bypass the compression middleware (?)
 &nbsp; &nbsp;response.headers.pop(<span class="hljs-string">'content-encoding'</span>, None)
 &nbsp; &nbsp;response.headers.pop(<span class="hljs-string">'Content-Encoding'</span>, None)
 &nbsp; &nbsp;
 &nbsp; &nbsp;<span class="hljs-keyword">return</span> HtmlResponse(
 &nbsp; &nbsp; &nbsp; &nbsp;page.url,
 &nbsp; &nbsp; &nbsp; &nbsp;status=response.status,
 &nbsp; &nbsp; &nbsp; &nbsp;headers=response.headers,
 &nbsp; &nbsp; &nbsp; &nbsp;body=body,
 &nbsp; &nbsp; &nbsp; &nbsp;encoding=<span class="hljs-string">'utf-8'</span>,
 &nbsp; &nbsp; &nbsp; &nbsp;request=request
 &nbsp;  )
</code></pre>















<p data-nodeid="218858">代码内容比较多，我们慢慢来说。</p>
<p data-nodeid="218859">首先最开始的部分是定义 Pyppeteer 的一些启动参数：</p>

<pre class="lang-java" data-nodeid="219376"><code data-language="java">options = {
 &nbsp; &nbsp;<span class="hljs-string">'headless'</span>: self.headless,
 &nbsp; &nbsp;<span class="hljs-string">'dumpio'</span>: self.dumpio,
 &nbsp; &nbsp;<span class="hljs-string">'devtools'</span>: self.devtools,
 &nbsp; &nbsp;<span class="hljs-string">'args'</span>: [
 &nbsp; &nbsp; &nbsp; &nbsp;f<span class="hljs-string">'--window-size={self.window_width},{self.window_height}'</span>,
 &nbsp;  ]
}
<span class="hljs-keyword">if</span> self.executable_path: options[<span class="hljs-string">'executable_path'</span>] = self.executable_path
<span class="hljs-keyword">if</span> self.disable_extensions: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--disable-extensions'</span>)
<span class="hljs-keyword">if</span> self.hide_scrollbars: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--hide-scrollbars'</span>)
<span class="hljs-keyword">if</span> self.mute_audio: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--mute-audio'</span>)
<span class="hljs-keyword">if</span> self.no_sandbox: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--no-sandbox'</span>)
<span class="hljs-keyword">if</span> self.disable_setuid_sandbox: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--disable-setuid-sandbox'</span>)
<span class="hljs-keyword">if</span> self.disable_gpu: options[<span class="hljs-string">'args'</span>].append(<span class="hljs-string">'--disable-gpu'</span>)
</code></pre>

<p data-nodeid="219891">这些参数来自 from_crawler 里面读取项目 settings 的内容，如配置 Pyppeteer 对应浏览器的无头模式、窗口大小、是否隐藏滚动条、是否弃用沙箱，等等。</p>
<p data-nodeid="219892">紧接着就是利用 options 来启动 Pyppeteer：</p>

<pre class="lang-java" data-nodeid="220411"><code data-language="java">browser = <span class="hljs-function">await <span class="hljs-title">launch</span><span class="hljs-params">(options)</span>
page </span>= await browser.newPage()
await page.setViewport({<span class="hljs-string">'width'</span>: self.window_width, <span class="hljs-string">'height'</span>: self.window_height})
</code></pre>

<p data-nodeid="220926">这里启动了 Pyppeteer 对应的浏览器，将其赋值为 browser，然后新建了一个选项卡，赋值为 page，然后通过 setViewport 方法设定了窗口的宽高。</p>
<p data-nodeid="220927">接下来就是对一些 Cookies 进行处理，如果 Request 带有 Cookies 的话会被赋值到 Pyppeteer 中：</p>

<pre class="lang-dart" data-nodeid="226594"><code data-language="dart"># <span class="hljs-keyword">set</span> cookies
<span class="hljs-keyword">if</span> isinstance(request.cookies, dict):
 &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.setCookie(*[
 &nbsp; &nbsp; &nbsp;  {<span class="hljs-string">'name'</span>: k, <span class="hljs-string">'value'</span>: v}
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> request.cookies.items()
 &nbsp;  ])
<span class="hljs-keyword">else</span>:
 &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.setCookie(request.cookies)
</code></pre>











<p data-nodeid="163624">再然后关键的步骤就是进行页面的加载了：</p>
<pre class="lang-java" data-nodeid="227109"><code data-language="java"><span class="hljs-keyword">try</span>:
 &nbsp; &nbsp;options = {
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string">'timeout'</span>: <span class="hljs-number">1000</span> * timeout,
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string">'waitUntil'</span>: request.wait_until
 &nbsp;  }
 &nbsp; &nbsp;logger.debug(<span class="hljs-string">'request %s with options %s'</span>, request.url, options)
 &nbsp; &nbsp;response = await page.goto(
 &nbsp; &nbsp; &nbsp; &nbsp;request.url,
 &nbsp; &nbsp; &nbsp; &nbsp;options=options
 &nbsp;  )
except (PageError, TimeoutError):
 &nbsp; &nbsp;logger.error(<span class="hljs-string">'error rendering url %s using pyppeteer'</span>, request.url)
 &nbsp; &nbsp;await page.close()
 &nbsp; &nbsp;await browser.close()
 &nbsp; &nbsp;<span class="hljs-keyword">return</span> self._retry(request, <span class="hljs-number">504</span>, spider)
</code></pre>

<p data-nodeid="227624">这里我们首先制定了加载超时时间 timeout 还有要等待完成的事件 waitUntil，接着调用 page 的 goto 方法访问对应的页面，同时进行了异常检测，如果发生错误就关闭浏览器并重新发起一次重试请求。</p>
<p data-nodeid="227625">在页面加载出来之后，我们还需要判定我们期望的结果是不是加载出来了，所以这里又增加了 waitFor 的调用：</p>

<pre class="lang-java" data-nodeid="228142"><code data-language="java"><span class="hljs-keyword">if</span> request.wait_for:
 &nbsp; &nbsp;<span class="hljs-keyword">try</span>:
 &nbsp; &nbsp; &nbsp; &nbsp;logger.debug(<span class="hljs-string">'waiting for %s finished'</span>, request.wait_for)
 &nbsp; &nbsp; &nbsp; &nbsp;await page.waitFor(request.wait_for)
 &nbsp; &nbsp;except TimeoutError:
 &nbsp; &nbsp; &nbsp; &nbsp;logger.error(<span class="hljs-string">'error waiting for %s of %s'</span>, request.wait_for, request.url)
 &nbsp; &nbsp; &nbsp; &nbsp;await page.close()
 &nbsp; &nbsp; &nbsp; &nbsp;await browser.close()
 &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword">return</span> self._retry(request, <span class="hljs-number">504</span>, spider)
</code></pre>

<p data-nodeid="228657">这里 request 有个 wait_for 属性，就可以定义想要加载的节点的选择器，如 <code data-backticks="1" data-nodeid="228662">.item .name</code> 等，这样如果页面在规定时间内加载出来就会继续向下执行，否则就会触发 TimeoutError 并被捕获，关闭浏览器并重新发起一次重试请求。</p>
<p data-nodeid="228658">等想要的结果加载出来之后，我们还可以执行一些自定义的 JavaScript 代码完成我们想要自定义的功能：</p>

<pre class="lang-dart" data-nodeid="234329"><code data-language="dart"># evaluate script
<span class="hljs-keyword">if</span> request.script:
 &nbsp; &nbsp;logger.debug(<span class="hljs-string">'evaluating %s'</span>, request.script)
 &nbsp; &nbsp;<span class="hljs-keyword">await</span> page.evaluate(request.script)
</code></pre>











<p data-nodeid="163630">最后关键的一步就是将当前页面的源代码打印出来，然后构造一个 HtmlResponse 返回即可：</p>
<pre class="lang-dart te-preview-highlight" data-nodeid="239994"><code data-language="dart">content = <span class="hljs-keyword">await</span> page.content()
body = str.encode(content)
​
# close page and browser
logger.debug(<span class="hljs-string">'close pyppeteer'</span>)
<span class="hljs-keyword">await</span> page.close()
<span class="hljs-keyword">await</span> browser.close()
​
<span class="hljs-keyword">if</span> not response:
 &nbsp; &nbsp;logger.error(<span class="hljs-string">'get null response by pyppeteer of url %s'</span>, request.url)
​
# Necessary to bypass the compression middleware (?)
response.headers.pop(<span class="hljs-string">'content-encoding'</span>, None)
response.headers.pop(<span class="hljs-string">'Content-Encoding'</span>, None)
​
<span class="hljs-keyword">return</span> HtmlResponse(
 &nbsp; &nbsp;page.url,
 &nbsp; &nbsp;status=response.status,
 &nbsp; &nbsp;headers=response.headers,
 &nbsp; &nbsp;body=body,
 &nbsp; &nbsp;encoding=<span class="hljs-string">'utf-8'</span>,
 &nbsp; &nbsp;request=request
)
</code></pre>











<p data-nodeid="164806">所以，如果代码可以执行到最后，返回到就是一个 Response 对象，这个 Resposne 对象的 body 就是 Pyppeteer  渲染页面后的结果，因此这个 Response 对象再传给 Spider 解析，就是 JavaScript 渲染后的页面结果了。</p>
<p data-nodeid="164807">这样我们就通过 Downloader Middleware 通过对接 Pyppeteer 完成 JavaScript 动态渲染页面的抓取了。</p>

---

### 精选评论


