<p data-nodeid="157412" class="">如果你曾经访问过 <a href="https://flink.apache.org/" data-nodeid="157416">Flink 官网</a>的话，你会看到 Flink 是这么描述它自己的：</p>

<p data-nodeid="158226" class=""><img src="https://s0.lgstatic.com/i/image6/M00/17/0D/CioPOWBHNWSAACawAAC1gtZB88o077.png" alt="Drawing 1.png" data-nodeid="158229"></p>



<p data-nodeid="155386">看到没，第一个词就是<strong data-nodeid="155519">Stateful</strong>（状态）！而且，正是因为这个 stateful，Flink 才会从众多开源流计算框架中脱颖而出，一下子就成为那个最靓的仔，还引得其他流计算框架开始纷纷效仿。</p>
<p data-nodeid="155387">那为什么“状态”对于流计算是如此重要呢？其实关于这个问题，大家也是在不断的实践中，才逐渐弄明白。可以这样说，只有理解了“状态”，才能够真正理解“流计算”。</p>
<p data-nodeid="155388">所以今天，我们就来详细讨论下实时流计算中有关于“状态”的问题吧！</p>
<h3 data-nodeid="155389">流的状态区分</h3>
<p data-nodeid="155390">说到流计算的“状态”，我们在前面讲解流计算的五类算法时，其实就已经接触过了。比如，关联操作中临时保存的窗口数据、时间维度聚合值计算时使用的寄存器、CEP 中的有限状态机、统计或机器学习模型的参数等，这些都是“状态”。</p>
<p data-nodeid="155391">但是，上面这些“状态”是有区别的！具体来说，这些“状态”可以分为两类，一类是<strong data-nodeid="155534">流数据状态，<strong data-nodeid="155533">另一类是</strong>流信息状态</strong>。下面我针对这两种状态分别解释下。</p>
<ul data-nodeid="155392">
<li data-nodeid="155393">
<p data-nodeid="155394">首先是<strong data-nodeid="155544">流数据状态</strong>。在流计算过程中，我们需要处理事件窗口、时间乱序、多流关联等问题。解决这些问题，通常需要对部分流数据进行临时缓存，并在计算完成时再将这些临时缓存清理掉。因此，我们将这些<strong data-nodeid="155545">临时保存的部分流数据称为“流数据状态”</strong>。</p>
</li>
<li data-nodeid="155395" class="">
<p data-nodeid="155396" class="">然后是<strong data-nodeid="155555">流信息状态</strong>。在流计算过程中，我们会得到一些有用的业务信息，比如时间维度的聚合值、关联图谱的一度关联节点数、CEP 的有限状态机等，这些信息会在后续被继续使用，从而需要将它们保存下来。同时在之后的流计算过程中，这些信息还会被不断地查询和更新。因此，我们<strong data-nodeid="155556">将这些分析所得并保存下来的业务信息称为“流信息状态”</strong>。</p>
</li>
</ul>
<p data-nodeid="159026" class=""><img src="https://s0.lgstatic.com/i/image6/M00/17/11/Cgp9HWBHNXCAR1kpAADJACD6xco814.png" alt="Drawing 3.png" data-nodeid="159029"></p>



<p data-nodeid="159814" class=""><strong data-nodeid="159819">为什么区分“流数据状态”和“流信息状态”非常重要呢？</strong> 思考这么一个问题，如果要计算“用户过去 7 天交易总金额”，你准备怎么做？</p>

<p data-nodeid="155401">一种显而易见的方法，是直接使用各种流计算框架提供的窗口函数来实现。比如在 Flink 中，就是如下代码：</p>
<pre class="lang-java" data-nodeid="155402"><code data-language="java">userTransactions
.keyBy(<span class="hljs-number">0</span>)
<span class="hljs-comment">// 滑动窗口，每1秒钟计算一次7天窗口内的交易金额</span>
.timeWindow(Time.days(<span class="hljs-number">7</span>), Time.seconds(<span class="hljs-number">1</span>))
.sum(<span class="hljs-number">1</span>);
</code></pre>
<p data-nodeid="155403">上面的 Flink 代码使用了 timeWindow 窗口，每 1 秒钟计算一次 7 天窗口内的总交易金额。其他流计算平台，像 Spark Streaming、Storm 等，也有类似的方法。<br>
但你一定会发现，上面的实现似乎有些奇怪。到底哪里奇怪呢？我认为至少有以下几点非常不妥。</p>
<ul data-nodeid="161427">
<li data-nodeid="161428">
<p data-nodeid="161429">一是，这个计算是每 1 秒钟才能输出结果。如果我们的业务需求是，<strong data-nodeid="161441">每来一个事件</strong>就对该事件所代表的用户，计算他在“过去 7 天交易的总金额”，那这种按照 1 秒钟做滑动窗口进行处理的方式，就与业务需求不相符了。</p>
</li>
<li data-nodeid="161430">
<p data-nodeid="161431">二是，窗口为 7 天，滑动步长为 1 秒，这两个时间相差的数量级过于巨大！这意味着对于同一份数据，在窗口滑动过程中，会被反复计算“7(天) ÷ 1(秒) ≈ 60万”次！当然，这里设置 1 秒是因为我们想尽可能地“实时”。如果觉得 1 秒太“过分”，你也可以设置滑动步长为 30 秒、60 秒等，但这并不能改变重复计算的本质，并且滑动步长越长，离“实时计算”越远。</p>
</li>
<li data-nodeid="161432">
<p data-nodeid="161433">三是，窗口为 7 天，就意味着在流计算系统中需要保存 7 天的数据。而我们想要计算的其实只是一个 sum 聚合值，保存 7 天的数据不仅会占用大量的内存和磁盘，还会显著降低处理速度。所以针对聚合值计算，还需要保存窗口内的全部数据，这显然是一个严重的问题。对于这个问题，一些流计算框架已经做了优化。比如 Flink 在计算 sum 一类的窗口聚合值时，默认是不用保存窗口内数据的，但是一旦用户需要做些定制化改动，则又退化到要保存窗口内全部数据的情况了。</p>
</li>
<li data-nodeid="161434">
<p data-nodeid="161435" class="">四是，如果我们要在一个事件上，计算<strong data-nodeid="161457">几十个</strong>类似于“用户过去 7 天交易总金额”这样的指标，按照 timeWindow 的实现方法，每个指标可能会有<strong data-nodeid="161458">不同的时间窗口</strong>和<strong data-nodeid="161459">滑动步长</strong>。如此一来，同步这几十个指标的计算过程并汇总它们的计算结果，也成了一件不容易的事情。</p>
</li>
</ul>


<p data-nodeid="155413">那么，是什么原因造成了以上这些不妥之处呢？这是因为，我们混淆了“数据窗口”和“业务窗口”。下面详细说明。</p>
<p data-nodeid="155414">在大多数开源流计算框架中，它们定义的“窗口”函数，其实是针对“流数据”的“分块”管理。我们用这些“窗口”函数，将原本“无穷无尽”的流数据，分割成一个个的“数据块”，然后在“数据块”上做各种计算。这种做法，本质上是对流数据的“分而治之”管理，所以我将这种窗口，称之为“数据窗口”。</p>
<p data-nodeid="155415">而在“用户过去 7 天交易总金额”的计算中，这里的“7 天”是属于业务意义上的时间窗口，所以我将这种窗口，称之为“业务窗口”。</p>
<p data-nodeid="155416">澄清了这两种不同的窗口后，我们再来分析下前面的 Flink 代码有什么问题。</p>
<p data-nodeid="155417">在前面的 Flink 代码中，我们直接使用 timeWindow(Time.days(7)，Time.seconds(1)) 来实现 “7 天”的时间窗口。这里的 Time.days(7) 属于 Flink 对“流数据”的“分块”管理，所以是“数据窗口”！我们将它设置为与“业务窗口”一致，其实是强行将“数据窗口”和“业务窗口”耦合起来。毕竟，我要实现“7 天”的“业务窗口”，为啥一定要用“7 天”的“数据窗口”来对流数据进行分块呢？</p>
<p data-nodeid="155418">比如，我先将交易数据保存到数据库中，然后将“数据窗口”设置为 1 小时按批处理。或者，我甚至可以根本就不使用“数据窗口”，而是逐个事件处理。这两种方法，它们的“数据窗口”都不是“7 天”，但它们都可以算出用户在“7 天”的“业务窗口”内交易的总金额！</p>
<p data-nodeid="155419">所以说，正是由于混淆了“数据窗口”与“业务窗口”，将它们强行耦和起来，才造成前面所说的各种不妥！</p>
<p data-nodeid="155420">接下来，我们来做更进一步的分析。也就是，如果从“状态”的角度看，“数据窗口”和“业务窗口”又有什么区别？</p>
<p data-nodeid="155421">在实现“数据窗口”时，我们工作的核心是<strong data-nodeid="155616">将部分流数据缓存起来</strong>，并对缓存的数据按“窗口”划分，最后再将窗口内的数据按批次处理。所以，<strong data-nodeid="155617">实现“数据窗口”所要保存的“状态”是流数据本身</strong>，这种“状态”就是“<strong data-nodeid="155618">流数据状态</strong>”。</p>
<p data-nodeid="155422">而在实现“业务窗口”时，我们工作的核心是<strong data-nodeid="155640">保存业务信息</strong>，比如<strong data-nodeid="155641">每天的交易总金额</strong>。这样当我们要计算 7 天的总交易金额时，只需要将这 7 天中<strong data-nodeid="155642">每天的交易总金额</strong>读取出来汇总即可。所以，<strong data-nodeid="155643">实现“业务窗口”所要保存的“状态”是流数据所含的业务信息</strong>，这种“状态”就是“<strong data-nodeid="155644">流信息状态</strong>”。</p>
<p data-nodeid="155423">经过上面的分析，你现在应该理解为啥要区分“流数据状态”和“流信息状态”了吧？<strong data-nodeid="155649">“流数据状态”和“流信息状态”根本就是两种不同的状态，“流数据状态”是在管理流数据过程中产生的状态，而“流信息状态”则是在分析流数据业务信息中产生的状态。</strong></p>
<p data-nodeid="155424">所以接下来，我就详细地讲解下这两种状态。</p>
<h3 data-nodeid="155425">流数据状态</h3>
<p data-nodeid="155426">我们先来看<strong data-nodeid="155657">流数据状态</strong>。说到流数据状态，其最重要的用途是实现“事件窗口”“时间乱序处理”和“流的关联”。下面我就这三种用途逐一讲解下。</p>
<h4 data-nodeid="155427">事件窗口</h4>
<p data-nodeid="155428">首先是“事件窗口”。</p>
<p data-nodeid="155429">在模块 2 时，我们实现过一个流计算框架。在这个框架中，事件处理的方式是来一个就处理一个，并没有“窗口”的概念。但在实际很多场景中，我们并不需要每来一个事件就处理一个事件，而是按照一定的间隔和窗口来处理事件。比如“每 30 秒钟计算一次过去五分钟交易总额”“每满 100 个事件计算平均交易金额”“统计用户在一次活跃期间点击过的商品数量”等。</p>
<p data-nodeid="155430">对于这些以“窗口”为单元来处理事件的方式，我们需要用一个缓冲区（buffer）临时存储过去一段时间接收到的事件。只有等到触发窗口计算的条件满足时，才开始处理窗口内的事件。最后当窗口里的数据被处理完时，还需要将以后无须再使用的数据清理掉。</p>
<p data-nodeid="155431">可以看出，以上“事件窗口”功能得以实现，是非常依赖于在缓冲区中保存部分流数据的。这种保存在缓冲区中的部分流数据，就是一种“流数据状态”。</p>
<h4 data-nodeid="155432">时间乱序</h4>
<p data-nodeid="155433">接下来是“时间乱序”。</p>
<p data-nodeid="155434">由于网络传输和并发处理的原因，在流计算系统接收到事件时，非常有可能事件已经在时间上乱序了。比如时间戳为 1532329665005 的事件，比时间戳为 1532329665001 的事件先到达流计算系统。怎样处理这种事件在时间上乱序的问题呢？通常的做法就是将收到的事件先保存起来，<strong data-nodeid="155670">等过一段时间</strong>后乱序的事件到达了，再将保存的事件按时间排序，这样就恢复了事件的时间顺序。</p>
<p data-nodeid="155435">当然，上面的过程存在一个问题，就是“<strong data-nodeid="155676">等过一段时间</strong>”到底是怎样等以及等多久？针对这个问题有一个非常优秀的解决方案，就是水印（watermark）。</p>
<p data-nodeid="155436">使用“水印”解决时间乱序的原理是这样的。在流计算数据中，按照一定的规律（比如以特定周期）插入“水印”，水印是一个时间戳。当处理单元接收到水印时，表示应该<strong data-nodeid="155690">处理所有时间戳比水印小的事件</strong>。我们通常将水印设置为“事件的时间戳<strong data-nodeid="155691">减去一段时间</strong>”所得的值，这样就给“先到的时间戳较大的事件”一个等待“晚到的时间戳较小的事件”的机会，而且确保了不会没完没了地等待下去。在这个过程中，等待时间的大小就是那个<strong data-nodeid="155692">减去的时间段</strong>了。</p>
<p data-nodeid="155437">不过，“水印”这种方案也不是百分百地解决了乱序问题，那些实在太晚到达的事件，就只能是过期不候了。另外，由于解决“时间乱序”的问题需要等待晚到的事件，所以不可避免地会对当前事件的处理带来一定延迟。</p>
<p data-nodeid="155438">总的来说，在使用“水印”解决“时间乱序”问题时，总是会将时间戳大于水印的数据先缓存起来。这些缓存的部分流数据，就是“流数据状态”。</p>
<h4 data-nodeid="155439">流的关联</h4>
<p data-nodeid="155440">最后是“流的关联”。</p>
<p data-nodeid="155441">在关系型数据库中，关联操作是一种非常普遍的行为，现在这种操作也被引进到流计算中来。比如 join、union 等，都是流计算中常用的关联操作。</p>
<p data-nodeid="155442">虽然“流数据”和“块数据”在数据形式和处理方式上都十分不同，但就“关联”操作而言，它们的工作原理是非常相似的。</p>
<p data-nodeid="155443">比如，流计算系统在实现 join 操作时，需要先将参与 join 的各个流在一段时间窗口内的全部数据都缓存起来。然后，以这些窗口内的数据为基础，做类似于关系型数据库中表与表之间的 join 计算。最后计算完成时，再将结果以流的方式输出。</p>
<p data-nodeid="155444">很显然，上面实现流数据 join 操作时，也是需要临时保存部分流数据的，因此“流的关联”操作也用到了“流数据状态”。</p>
<p data-nodeid="155445">除了以上三种主要用途外，流计算系统在实现其他一些功能时，也需要缓存部分的流数据，比如排序（sorting）、分组（group by）等。因此，这些功能也都使用到了“流数据状态”。</p>
<p data-nodeid="155446">接下来，我们再看下有关“流数据状态”的存储问题。</p>
<h4 data-nodeid="155447">流数据状态的存储</h4>
<p data-nodeid="155448">“流数据状态”最理想的情况是全部保存在内存中，只有在做持久化（checkpoint）时，才写入磁盘。这样做的原因在于，流数据从接收、处理到删除的过程，具有实时、快速和临时的特点，如果每次接收到一个新事件，都要将其写入磁盘，势必会引起性能的急剧下降。</p>
<p data-nodeid="155449">但是，将所有数据全都放在内存，终究还是太过理想。因为大多数场景下，“数据窗口”内的数据量都超过了内存容量。所以，此时也可以将“流数据状态”存放在文件或其他外部存储系统中。这样会对性能造成一定的影响，但避免了内存对数据量的限制。</p>
<p data-nodeid="155450">至此，我们就讨论完所有“流数据状态”相关的内容了。总的来说，“流数据状态”最重要的功能是实现窗口、关联和乱序处理。在后续模块四的课时中，我还会针对各种开源流计算框架，讲解它们对“流数据状态”的支持情况，这里就不再继续展开了。</p>
<h3 data-nodeid="155451">流信息状态</h3>
<p data-nodeid="155452">接下来，我们再来看<strong data-nodeid="155713">流信息状态</strong>。“流信息状态”最主要的功能，是记录在流计算过程中分析出的业务信息。</p>
<p data-nodeid="155453">还是拿前面“用户过去 7 天交易总金额”的例子来说。在实时风控系统中，通常会要求针对每一个到来的交易事件，计算该交易事件的用户在过去 7 天交易的总金额。这种情况下，我们可以将每天的交易金额记录为一条“状态”。当一个交易事件到来时，只需要将过去 7 天每天的交易金额“状态”读取出来，然后汇总求和，就得到了 7 天的总交易金额。</p>
<p data-nodeid="155454">在上面这个例子中，<strong data-nodeid="155720">将每天的交易金额记录为一条“状态”，就是我们说的“流信息状态”</strong>。</p>
<p data-nodeid="155455"><strong data-nodeid="155725">流信息状态的存储，通常是依赖于数据库完成的</strong>。这有三方面的原因。</p>
<ul data-nodeid="155456">
<li data-nodeid="155457">
<p data-nodeid="155458">一是，“流信息状态”通常需要保存较长时间，数据量也不小，还需要频繁查询和更新，将它存放在数据库中，能方便地长期保存和增删查改。</p>
</li>
<li data-nodeid="155459">
<p data-nodeid="155460">二是，“流信息状态”存在“数据变冷”和“过期淘汰”的问题，使用数据库的“热数据缓存”和“TTL 机制”，能方便有效地解决这两个问题。</p>
</li>
<li data-nodeid="155461">
<p data-nodeid="155462">三是，“流信息状态”通常数据量会很大，单个存储节点往往是不够用的，选择合适的数据库能够方便地扩展为集群。</p>
</li>
</ul>
<p data-nodeid="155463">以上就是我们选择用数据库来存储“流信息状态”的原因了。</p>
<p data-nodeid="155464">接下来我们就来看看，具体如何用数据库存储“流信息状态”吧！</p>
<h4 data-nodeid="155465">使用 Apache Ignite 存储流信息状态</h4>
<p data-nodeid="155466">实际上，我们在<a href="https://kaiwu.lagou.com/course/courseInfo.htm?courseId=614&amp;sid=20-h5Url-0&amp;buyFrom=2&amp;pageId=1pz4#/detail/pc?id=6427" data-nodeid="155735"> 10 课时中</a>，已经使用 Redis 数据库来存储“流信息状态”了。当时，我们为了计算“过去一周内在相同设备上交易的次数”，使用 Redis 的 INCR 指令来实现了计数功能，将记录交易次数信息的寄存器，存放在 Redis 中。这个过程其实就是在使用 Redis 存储“流信息状态”。</p>
<p data-nodeid="155467">不过现在，我们使用另外一种不同的方案来存储“流信息状态”，这就是 Apache Ignite。这里之所以要“多此一举”地用两种不同的方案来实现相同的功能，也是有重要原因的。具体为什么，你在下一个课时就会知道了。</p>
<p data-nodeid="155468">这里我重点讲解如何用 Apache Ignite 存储“流信息状态”。</p>
<p data-nodeid="155469">Apache Ignite 是一种<strong data-nodeid="155744">基于内存的数据网格</strong>方案，也可以用作分布式内存数据库。它提供了符合 JCache 标准的数据访问接口，支持丰富的数据结构，并且支持 SQL 查询功能。</p>
<p data-nodeid="155470">为了更好地对比，我这里还是计算“过去一周在相同设备上交易次数”。下面就是具体的实现过程。</p>
<p data-nodeid="155471">首先，我们需要定义一个用于存储“流信息状态”的“表”，这个“表”是数据存储的格式。具体定义如下：</p>
<pre class="lang-java" data-nodeid="155472"><code data-language="java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CountTable</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">Serializable</span> </span>{
&nbsp; &nbsp; <span class="hljs-meta">@QuerySqlField(index = true)</span>
&nbsp; &nbsp; <span class="hljs-keyword">private</span> String name;
&nbsp; &nbsp; <span class="hljs-meta">@QuerySqlField(index = true)</span>
&nbsp; &nbsp; <span class="hljs-keyword">private</span> <span class="hljs-keyword">long</span> timestamp;
&nbsp; &nbsp; <span class="hljs-meta">@QuerySqlField</span>
&nbsp; &nbsp; <span class="hljs-keyword">private</span> <span class="hljs-keyword">double</span> amount;
&nbsp; &nbsp; <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">CountTable</span><span class="hljs-params">(String name, <span class="hljs-keyword">long</span> timestamp, <span class="hljs-keyword">double</span> amount)</span> </span>{
&nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-keyword">this</span>.name = name;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-keyword">this</span>.timestamp = timestamp;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-keyword">this</span>.amount = amount;
&nbsp; &nbsp; }
&nbsp; &nbsp; <span class="hljs-comment">// 必需重写equals方法，否则在经过序列化和反序列化后，Ignite会视为不同记录，实际上它们是同一条记录</span>
&nbsp; &nbsp; <span class="hljs-meta">@Override</span>
&nbsp; &nbsp; <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">equals</span><span class="hljs-params">(Object o)</span> </span>{
&nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-keyword">if</span> (<span class="hljs-keyword">this</span> == o) <span class="hljs-keyword">return</span> <span class="hljs-keyword">true</span>;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-keyword">if</span> (o == <span class="hljs-keyword">null</span> || getClass() != o.getClass()) <span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;
&nbsp; &nbsp; &nbsp; &nbsp; CountTable that = (CountTable) o;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-keyword">if</span> (timestamp != that.timestamp) <span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-keyword">if</span> (Double.compare(that.amount, amount) != <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> <span class="hljs-keyword">false</span>;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-keyword">return</span> name != <span class="hljs-keyword">null</span> ? name.equals(that.name) : that.name == <span class="hljs-keyword">null</span>;
&nbsp; &nbsp; }
&nbsp; &nbsp; <span class="hljs-comment">// 因为重写了equals方法，所以hashCode()方法也跟着一起重写</span>
&nbsp; &nbsp; <span class="hljs-meta">@Override</span>
&nbsp; &nbsp; <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">hashCode</span><span class="hljs-params">()</span> </span>{
&nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-keyword">int</span> result;
&nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-keyword">long</span> temp;
&nbsp; &nbsp; &nbsp; &nbsp; result = name != <span class="hljs-keyword">null</span> ? name.hashCode() : <span class="hljs-number">0</span>;
&nbsp; &nbsp; &nbsp; &nbsp; result = <span class="hljs-number">31</span> * result + (<span class="hljs-keyword">int</span>) (timestamp ^ (timestamp &gt;&gt;&gt; <span class="hljs-number">32</span>));
&nbsp; &nbsp; &nbsp; &nbsp; temp = Double.doubleToLongBits(amount);
&nbsp; &nbsp; &nbsp; &nbsp; result = <span class="hljs-number">31</span> * result + (<span class="hljs-keyword">int</span>) (temp ^ (temp &gt;&gt;&gt; <span class="hljs-number">32</span>));
&nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-keyword">return</span> result;
&nbsp; &nbsp; }
}
</code></pre>
<p data-nodeid="155473">在上面的表定义中：</p>
<ul data-nodeid="155474">
<li data-nodeid="155475">
<p data-nodeid="155476">name 用于记录状态的关键字；</p>
</li>
<li data-nodeid="155477">
<p data-nodeid="155478">timestamp 用于记录事件处理时的时间戳；</p>
</li>
<li data-nodeid="155479">
<p data-nodeid="155480">amount 用于记录状态发生的次数。</p>
</li>
</ul>
<p data-nodeid="155481">另外，我重写了表 CountTable 类的 equals 方法和 hashCode 方法，这是因为 Apache Ignite 在执行 replace 这类方法时，会进行对象的比较。而由于 Apache Ignite 本身是一个分布式系统，在查询过程中会涉及对象序列化和反序列化的过程。这个时候如果不重写 equals 方法，会导致原本字段完全一样的记录会被视为不同记录，使得程序运行错误。</p>
<p data-nodeid="155482">与用 Redis 时的实现思路完全一致，我也将 7 天的时间窗口划分为 7 个小窗口，每个小窗口代表 1 天。在每个小窗口内，分配一个用来记录这个窗口事件数的关键字，也就是 CountTable 表定义中的 name 字段，name 取值的格式如下：</p>
<pre class="lang-python" data-nodeid="155483"><code data-language="python">$event_type.$device_id.$window_unit.$window_index
</code></pre>
<p data-nodeid="162248">其中，“$event_type”表示事件类型，“$device_id”表示设备id，“$window_unit”表示时间窗口单元，“$window_index”表示时间窗口索引。</p>
<p data-nodeid="162249">比如，对于“device_id”为“d000001”的设备，如果在时间戳为“1532496076032”的时刻更新窗口，则计算的伪代码如下：</p>

<pre class="lang-python" data-nodeid="155485"><code data-language="python">$event_type = <span class="hljs-string">"transaction"</span>
$device_id = <span class="hljs-string">"d000001"</span>
$window_unit = <span class="hljs-number">86400000</span>&nbsp; <span class="hljs-comment"># 时间窗口单元为1天，即86400000毫秒</span>
$window_index = <span class="hljs-number">1532496076032</span> / $window_unit = <span class="hljs-number">17737</span>&nbsp; &nbsp; <span class="hljs-comment"># 用时间戳除以时间窗口单元，得到时间窗口索引</span>
$atTime ＝ ($window_index + <span class="hljs-number">1</span>) * $window_unit
$name = <span class="hljs-string">"$event_type.$device_id.$window_unit.$window_index"</span>
$cache = ignite.getOrCreateCache()
$id = md5($name);
$newRecord = new CountTable($name, $atTime, <span class="hljs-number">1</span>);
do {
&nbsp; &nbsp; $oldRecord = $cache.get($id);
&nbsp; &nbsp; <span class="hljs-keyword">if</span> ($oldRecord != null) {
&nbsp; &nbsp; &nbsp; &nbsp; $newRecord.amount = $oldRecord.amount + <span class="hljs-number">1</span>;
&nbsp; &nbsp; } <span class="hljs-keyword">else</span> {
&nbsp; &nbsp; &nbsp; &nbsp; $oldRecord = $newRecord;
&nbsp; &nbsp; &nbsp; &nbsp; $cache.putIfAbsent($id, oldRecord);
&nbsp; &nbsp; }
&nbsp; &nbsp; $succeed = $cache.replace($id, $oldRecord, $newRecord);
} <span class="hljs-keyword">while</span> (!$succeed);
</code></pre>
<p data-nodeid="163050">上面的伪代码描述了使用 Apache Ignite 的 JCache 接口更新某个窗口计数的方法，它实现的功能与之前用 Redis 时并无二致。</p>
<p data-nodeid="163051">但是，由于 Apache Ignite 没有提供类似于 Redis 中 INCR 指令那样的原子加操作，所以需要自行实现并发安全的累加操作。这里我没有采用“锁”的方案，而是使用了 CAS（Compare And Swap）。CAS 是一种无锁机制，在高并发场景下会比传统的锁具备更好的性能表现。在上面代码的“do while”循环部分，就是 CAS 的实现，其中“$cache.replace”是一个原子操作，保证了 CAS 的并发安全。</p>

<p data-nodeid="155487">在更新完子窗口的计数后，就是查询完整窗口的总计数了。我们只需要对子时间窗口内的计数做查询并汇总即可。具体实现如下：</p>
<pre class="lang-python" data-nodeid="155488"><code data-language="python">$event_type = <span class="hljs-string">"transaction"</span>
$device_id = <span class="hljs-string">"d000001"</span>
$window_unit = <span class="hljs-number">86400000</span>&nbsp; <span class="hljs-comment"># 时间窗口单元为1天，即86400000毫秒</span>
$window_index = <span class="hljs-number">1532496076032</span> / $window_unit = <span class="hljs-number">17737</span>&nbsp; &nbsp; <span class="hljs-comment"># 用时间戳除以时间窗口单元，得到时间窗口索引</span>
$atTime ＝ ($window_index + <span class="hljs-number">1</span>) * $window_unit
$startTime = $atTime - $window_unit * <span class="hljs-number">7</span>;&nbsp; &nbsp; <span class="hljs-comment"># 窗口为7天</span>
$name = <span class="hljs-string">"$event_type.$device_id.$window_unit.$window_index"</span>
$cache = ignite.getOrCreateCache()
$sumQuery = <span class="hljs-string">"SELECT sum(amount) FROM CountTable "</span> +&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span class="hljs-string">"WHERE name = $name and timestamp &gt; $startTime and timestamp &lt;= $atTime"</span>;
sum = $cache.query($sumQuery)
<span class="hljs-keyword">return</span> sum
</code></pre>
<p data-nodeid="155489">上面的伪代码充分利用了 Apache Ignite 支持 SQL 查询的便利性，很容易地计算出过去 7 天交易的总次数。</p>
<p data-nodeid="155490">至此，我们就使用 Apache Ignite 实现了“过去一周在相同设备上交易次数”的计算。</p>
<p data-nodeid="155491">总的来说，对于“流信息状态”，我们通常是使用数据库来进行管理，比如 Redis、Apache Ignite、RocksDB 等。我们一般还会要求这些数据库是高性能的，并且可以扩展为集群。对于扩展为集群这点，我会在下一个课时重点讨论。</p>
<h3 data-nodeid="155492">小结</h3>
<p data-nodeid="155493">今天，我们讨论了实时流计算应用中状态管理的问题，并将实时流计算应用中的状态分为了“流数据状态”和“流信息状态”。</p>
<p data-nodeid="155494">可以说，“流数据状态”和“流信息状态”分别是从两个不同的维度对“流”进行的管理。<strong data-nodeid="155780">前者“流数据状态”是从“时间”角度对流进行管理，而后者“流信息状态”则是从“空间”角度对流进行管理。</strong>“流信息状态”弥补了“流数据状态”只是对事件在时间序列上做管理的不足，将流的状态扩展到了任意的空间。</p>
<p data-nodeid="155495">将“流数据状态”和“流信息状态”这两个概念区分开，会指引我们将“流计算应用本身的执行过程”和“流数据的信息管理机制”解耦，这使得实时流计算系统的整体结构更加清晰。</p>
<p data-nodeid="155496">今天的课程内容稍微偏长了，这是因为“状态”问题对于流计算系统来说，实在是太过重要的内容。它是你以后能否灵活运用各种流计算框架解决业务问题的关键所在，所以请你务必掌握好今天的内容。</p>
<p data-nodeid="155497">最后，我们留一个小作业。对于“过去 1 小时成交总金额”和“过去 3 个月成交总金额”这两个计算任务，用 Flink 的话你会分别怎样实现呢？可以将你的想法或问题写在留言区。</p>
<p data-nodeid="155498">下面是本课时的知识脑图，以便于你理解。</p>
<p data-nodeid="163842" class="te-preview-highlight"><img src="https://s0.lgstatic.com/i/image6/M00/17/11/Cgp9HWBHNbqAFSWxAA0fYlDmXYM226.png" alt="Drawing 4.png" data-nodeid="163845"></p>

---

### 精选评论

##### *翔：
> 流信息状态保存每一天聚合结果,新的一天统计最近7天总金额时,取出前6天结果和新的一天结果做聚合,得到最新的7天总金额,需求换成每10S统计最近1小时总金额,开大小10S步长10S的滑动窗口,结果存到Redis,要统计最近一小时数据时,直接到Redis通过Key找到若干个10S结果的和就能求到最近1小时总金额了,不知道理解的对不对.

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 你的理解非常正确哈！看来你是完全掌握流信息状态和流数据状态的区别了，给你点赞！统计7天总金额时，使用的就是流信息状态，存放在Keyed State里即可。每10S统计最近1小时总金额时，使用的是流数据状态，直接使用Flink的窗口功能即可，计算结果再存放在Redis中，供外部服务查询。

##### *域：
> 意思是，我们更应该关注流信息状态，用以实现我们的业务需求。不应该以流数据状态为标准来实现需求吗？实际上流数据处理更应该以效率速度为基准，变向实现跨度大的伪实时需求。

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 流数据状态和流信息状态都很重要，只不过它们有各自的用途哈。比如对于“每1分钟统计一次过去1小时总交易金额”和“每来一个事件就统计一次该事件对应用户过去7天总交易金额”这两个不同的计算任务，前者就更适合使用流数据状态，直接使用Flink的滑动窗口实现即可。而后者更适合使用流信息状态，需要用到Keyed State来记录每个用户各天的交易金额。所以使用哪种状态，还是需要根据业务场景来选择。

##### **3430：
> 意思是，用户过去 7 天交易总金额这个例子，可以改成window为一个小时，然后在窗口内对触发交易的这些用户先查数据库拿近七天每天的交易金额，然后汇总一下返回，并更新最新的数据到数据库里

