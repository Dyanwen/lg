<p data-nodeid="5501" class="">专题分析之后往往会有很多落地项，在落地项的基础上，很多时候要进行 A/B 测试。今天我就讲下 A/B 测试。</p>
<p data-nodeid="5502">本课时内容分为三部分：</p>
<ul data-nodeid="5503">
<li data-nodeid="5504">
<p data-nodeid="5505">A/B 测试介绍；</p>
</li>
<li data-nodeid="5506">
<p data-nodeid="5507">A/B 测试注意事项；</p>
</li>
<li data-nodeid="5508">
<p data-nodeid="5509">A/B 测试案例。</p>
</li>
</ul>
<h3 data-nodeid="5510">A/B 测试介绍</h3>
<h4 data-nodeid="5511">A/B测试概念</h4>
<p data-nodeid="5512">我们先看下 A/B 测试的介绍。我这里直接借用百度百科的定义，然后在官方定义的基础上阐述说明，如下所示。</p>
<p data-nodeid="5513"><img src="https://s0.lgstatic.com/i/image/M00/3C/FF/CgqCHl8o-OGAdhLyAADPKWN2d3c736.png" alt="Drawing 0.png" data-nodeid="5642"></p>
<p data-nodeid="5514">A/B 测试是为 Web 和 App 界面或流程制作两个（A/B）或多个（A/B/n）版本，在同一时间维度，分别让组成成分相同（相似）的访客群组（目标人群）随机的访问这些版本，收集各群组的用户体验数据和业务数据，最后分析、评估出最好版本，正式采用。这里面 有几个关键词。</p>
<p data-nodeid="5515">第一个是<strong data-nodeid="5649">组成成分相同的访客</strong>。这说明用户群一定要一样，这个是在做 A/B 测试时最容易犯的一个错误，比如 A 组的用户明显要活跃于 B 组的用户，那最后 A 的数据肯定要比 B 好。</p>
<p data-nodeid="5516">第二个是<strong data-nodeid="5655">同一时间</strong>。A/B 测试对比的时候，一定是在同一个时间段进行对比，否则就没有意义。因为在时间因素上，我们没有办法去控制一定要是同一个时间段。我后面会举一个日常大家容易犯错的例子。</p>
<p data-nodeid="5517">第三个就是<strong data-nodeid="5661">用户体验数据和业务数据</strong>。在做 A/B 测试时，都要提前搭建好 A/B 测试的整套指标体系，这个指标体系肯定是业务数据分析师去跟进。</p>
<p data-nodeid="5518">这就是一个整体的概念，实际上 A/B 测试时通常有几个版本，然后每一个版本只有一个变量的变动。然后在变量变动基础上，在同一时间然后让一些用户流入进来，然后看各个用户最后的体验数据或业务数据，最后看哪个版本比较好。</p>
<p data-nodeid="5519">A/B 测试的整体流程一般是分为以下几步：</p>
<ol data-nodeid="5520">
<li data-nodeid="5521">
<p data-nodeid="5522">根据数据分析得到某建议项。很多算法同学每天也做大量的 A/B 测试，但效果往往都不太好。因为很多时候算法同学都是在凭感觉在调仓，不是说不对，只不过不是特别精准，最好是根据数据分析得到某建议项。</p>
</li>
<li data-nodeid="5523">
<p data-nodeid="5524">有了建议项之后，产品经理直接就落地了吗？并没有，落地一般要通过 A/B 测试。</p>
</li>
<li data-nodeid="5525">
<p data-nodeid="5526">根据某落地项，研发和设计人员进行开发设计（往往是先设计，然后再丢到 A/B 测试平台里面跑数据）。</p>
</li>
<li data-nodeid="5527">
<p data-nodeid="5528">研发人员数据采集，这一步现在基本都是自动采集数据，所以一般就不用管。</p>
</li>
<li data-nodeid="5529">
<p data-nodeid="5530">分析师跟进 A/B 测试效果，当显著性在 95% 以上并且维持了一段时间，实验就可以结束了。</p>
</li>
</ol>
<p data-nodeid="5531">整体节奏要按照灰度、5%、10%、20%、50%、100% 来控制。灰度是什么意思？通俗讲就是小版本。比如我们是一个千万量级的 Apple，灰度代表 5~10w 这样的一个水平。</p>
<p data-nodeid="5532">为什么要按照这样的节奏，我解释一下。比如当你发现某一个优化项好像还可以，然后你直接对版本进行优化。万一这个版本改过之后，数据不太好，比如说降低了 20% 的用户活跃度，这时候就会造成非常大的影响。所以做 A/B 测试的时候，一定是先灰度，采用一小部分用户。当我们灰度测试效果不错时，我们再放量，比如说放到 5%，当 50% 也不错的时候再放到 100%，总之一定要控制节奏。目前 A/B 测试，业界都是一套 A/B 测试平台，大公司可能会自己研发，小公司自己购买即可。</p>
<h4 data-nodeid="5533">常见的两种A/B测试类型</h4>
<h5 data-nodeid="5534">1. UI 界面型</h5>
<p data-nodeid="5535">以墨迹天气 App 为例,如图所示。</p>
<p data-nodeid="5536"><img src="https://s0.lgstatic.com/i/image/M00/3C/FF/CgqCHl8o-P-AS5_7AAIk-SAHILQ957.png" alt="Drawing 1.png" data-nodeid="5680"></p>
<p data-nodeid="5537">这里放了一个小人，以小人为例，在产品设计之初，要不要增加一个小人只是一个想法，而这个必须要经过 A/B 测试才能决定要不要实现。因此 A 版本没有小人，B 版本有小人，结果是 B 版本的数据比 A 版本要好，所以最终都有小人。这里提示一点，所有的设计师都要有 A/B 测试的思想才能更棒，往往你认为好看或者好用都不是很靠谱。</p>
<h5 data-nodeid="5538">2. 算法策略型</h5>
<p data-nodeid="5539">针对用户的内容推荐，以小红书 App 为例。当新用户下载完小红书，然后进行注册之后，就会发现有一个新区域需要用户选择感兴趣的内容，如图所示。</p>
<p data-nodeid="5540"><img src="https://s0.lgstatic.com/i/image/M00/3C/FF/CgqCHl8o-Q6Ac-kbAAIUPVaD3ik281.png" alt="Drawing 3.png" data-nodeid="5688"></p>
<p data-nodeid="5541">当用户选择完感兴趣的内容之后，小红书会用自身的算法给用户在首屏做推荐，这个时候也是很讲究的。比如 A 策略就是 100% 的兴趣预选，你选什么，我在首屏我就给你推什么。而 B 策略就是你选什么，我推的 80% 的内容是兴趣预选，剩下的 20% 是我随机分发，或者推荐一些热点的内容。这两种策略很不一样。</p>
<p data-nodeid="5542">那么到底是 A 策略好还是 B 策略好呢，A/B 测试后，比如说 B 策略要明显好于 A 策略，那我们接下来就可以针对新用户先试用 B 策略。实际上对于任何一款个性化内容的 App，给用户的推荐都涉及大量的算法策略型 A/B 测试，这里不一定就只是 A/B 两个策略，很可能还有一个 C 策略。还有一点，A 、B 两组的样本都要在 10w 以上才可以初步看数据。</p>
<h4 data-nodeid="5543">实际工作中的问题</h4>
<p data-nodeid="5544">严格模式下，所有的专题报告落地项（除了明显的 Bug 修复和明显的用户体验）都要靠 A/B 测试展开，然而分析师经常会遇到这种问题。</p>
<p data-nodeid="5545">举个例子：2 个月前，产品上线了短视频功能，2 个月后，大盘略涨（之前是略跌趋势），短视频和非短视频的数据增加也明显，现在短视频业务方希望分析师能量化出——大盘的上涨主要是因为短视频带来的。</p>
<p data-nodeid="5546">实际上这种问题非常头疼，因为你怎么解释都解释不清，往往一些分析师的思路就是选同一批用户，然后对比这一批用户在使用短视频前后的数据。那么这实际上就违背了我们前面的原则——同一时间。真正的 A/B 测试一定是同一个时间维度，你这里既有前又有后，时间因素上说不过去。所以针对这种问题，实际上确实只能靠 A/B 测试解决，但在当你准备上线短视频功能时，你就要开始做 A/B 测试，而不是说等数据表现很好时，你才想起来这件事，这就太迟了。</p>
<h3 data-nodeid="5547">A/B 测试注意事项</h3>
<p data-nodeid="5548">站在数据分析师的角度去看，A/B 测试时要注意以下事项：</p>
<ol data-nodeid="5549">
<li data-nodeid="5550">
<p data-nodeid="5551">A/B 两个组是否真的相同——虽然研发负责搭建，但分析师要知道大概原理；</p>
</li>
<li data-nodeid="5552">
<p data-nodeid="5553">策略是否生效——研发说进行了 A/B 测试，但分析师要去抽样看；</p>
</li>
<li data-nodeid="5554">
<p data-nodeid="5555">A/B 测试评估指标体系——要在 A/B 测试之前，就与研发沟通好看哪些综合性指标；</p>
</li>
<li data-nodeid="5556">
<p data-nodeid="5557">多观察几天数据——往往前几天数据可能有点问题，一般 3 天后数据才可正式使用；</p>
</li>
<li data-nodeid="5558">
<p data-nodeid="5559">A/B 测试的存档规划——所有 A/B 都要文档化，方便后续找增长点。</p>
</li>
</ol>
<p data-nodeid="5560">下面我们逐一来看。</p>
<h4 data-nodeid="5561">1. A/B 两个组是否真的相同，只存在一个变量？如下所示。</h4>
<ul data-nodeid="5562">
<li data-nodeid="5563">
<p data-nodeid="5564">A：001 002 003 004 005</p>
</li>
<li data-nodeid="5565">
<p data-nodeid="5566">B：001 002 003 004</p>
</li>
<li data-nodeid="5567">
<p data-nodeid="5568">C：001 002 003 004 006</p>
</li>
<li data-nodeid="5569">
<p data-nodeid="5570">D：X Y 001 002 003 004</p>
</li>
</ul>
<p data-nodeid="5571">其中，A，B，C 是可以做 A/B 测试的，但是 D 不行，一定要确保只有一个变量，然后通过最终数据来看这个变量是正向还是负向效应。实际工作过程中，研发可能会说只有一个变量，但还真不一定，所以分析师在做这件事的时候，可以把 A/B 两个组的原始日志中的分组标志抽出来，看下有无问题。由于每周都会有大量的 A/B 测试，所以一定要保证 A/B 两组只有一个变量不同。</p>
<h4 data-nodeid="5572">2. 策略是否生效</h4>
<p data-nodeid="5573">工作中常见这种现象，产品经理根据分析师的专题报告落地项 X，然后进行某个 A/B 测试，研发也进行了 A/B 测试，最后发现效果不明显，此时所有人都觉得 X 优化项没用，也就没有多去做更多尝试。</p>
<p data-nodeid="5574">这时候分析师一定要去对 A/B 组进行抽样，看 B 组（实验组）的用户是否真的上线了 X 优化，验证策略是否生效。A/B 测试系统本身就很复杂，出问题是非常正常的，虽然我们不一定要很了解内部详细原理，但是要知道有没有明显问题。</p>
<h4 data-nodeid="5575">3. A/B 测试评估指标体系</h4>
<p data-nodeid="5576">在 A/B 测试之前，就要考虑好最终要有哪些指标来评估效果，最好是能设计出一套综合性的指标体系，后续做实验直接看报表数据即可，不用每次单独建表。比如，我们可以是这种格式，如下图所示。</p>
<p data-nodeid="5577"><img src="https://s0.lgstatic.com/i/image/M00/3D/00/CgqCHl8o-V2AazTMAABHpV8mkFI238.png" alt="Drawing 5.png" data-nodeid="5722"></p>
<p data-nodeid="5578">报表格式包含实验策略、用户数、时间周期、次留、时长、点击率等因素，这样以后就很方便。</p>
<h4 data-nodeid="5579">4. 多观察几天数据</h4>
<p data-nodeid="5580">一般而言，前 3 天都是处于一个实验阶段，数据往往参考价值不大。而第 4~10 天，数据相对比较稳定，可以当作测试结论。</p>
<p data-nodeid="5581"><img src="https://s0.lgstatic.com/i/image/M00/3D/00/CgqCHl8o-XGAWS-NAACu_PJ_mN8269.png" alt="Drawing 7.png" data-nodeid="5732"></p>
<p data-nodeid="5582">以上述图表为例，我们可以看出在前 3 天（7月 1 号到 7 月 3 号），实验组的数据与对照组相比还差一点，或者说就是处于一个波动状态。而在 7 月 4 号到 7 月 7 号之后，实验组的数据明显要好于对照组的数据，所以一定要多观察几天数据。</p>
<p data-nodeid="5583">很多人做 A/B 测试，发现前面三天数据不好，后面就不看了，这是不可以的。我们好不容易出来一个落地项，又好不容易推动各方资源去做了一个 A/B 测试，最后你看的时候就很马虎，这就很不应该。所以要多观察几天数据，一般是 4~10 天，一周左右的时间即可。</p>
<h4 data-nodeid="5584">5. A/B 测试存档</h4>
<p data-nodeid="5585">分析师要定期复盘做了哪些 A/B 测试，以及它的预期效果和实际效果，这就是落地项的闭环。为什么这一页我要单独拿出来，是因为分析师在述职报告时，又或者是跟业务方同步最近做了哪些事比较有意义时，分析师就要把 A/B 测试的效果讲出来。 这里就建议采用 5W1H 方法来管理A/B 测试，如图所示。</p>
<p data-nodeid="5586"><img src="https://s0.lgstatic.com/i/image/M00/3C/F5/Ciqc1F8o-YCAW5M2AABAjAiEzIg881.png" alt="Drawing 8.png" data-nodeid="5743"></p>
<p data-nodeid="5587">比如，A/B 测试项的具体内容是什么？为什么要测试？测试时间周期是什么时候？测试负责人是谁？预期效果是怎样？实际效果又是怎样。如果实际效果不好，甚至可能还要写出大概是什么原因。文档是数据分析师日常非常重要的工作之一，一定要标准化，规范化。你可以写得非常详细，非常易懂，这样业务方每问你一次，你直接丢他一个链接即可。</p>
<h3 data-nodeid="5588">A/B 测试案例</h3>
<h4 data-nodeid="5589">Netflix 海报案例</h4>
<p data-nodeid="5590">我们看一下 Netflix 的一个案例。Netflix 在 2013 年做过一个实验，用来研究不同的海报风格对观看者数量的影响。当时他们就以《The Short Game》电影为例，设计了三个封面，如下所示。</p>
<p data-nodeid="5591"><img src="https://s0.lgstatic.com/i/image/M00/3C/F5/Ciqc1F8o-Y-AbVZ5AARoqBwibC0754.png" alt="Drawing 10.png" data-nodeid="5750"></p>
<p data-nodeid="5592">后来发现第二张图的观众量要比第一张图高 14%，当有这个结论的时候，他们就很兴奋。后来他们就开发了一套 A/B 测试系统，能够自动将具有相同背景，包括你的长度、宽度、剪辑、装饰、标题等元素的图像组合在一起，拼成一幅海报，然后分别测试用户对每张海报的点击转化率和后续行为。</p>
<p data-nodeid="5593">像《驯龙高手》这部电影，他们发现用户很喜欢第二张图和第三张图的封面。</p>
<p data-nodeid="5594"><img src="https://s0.lgstatic.com/i/image/M00/3D/00/CgqCHl8o-ZyAAZskAA4QxU7z99g170.png" alt="Drawing 12.png" data-nodeid="5755"></p>
<p data-nodeid="5595">而对于《我本坚强》美剧发现，如下图所示。当用户有面部表情，比如最后有一张比较好的面部表情的时候，用户也是非常喜欢的。</p>
<p data-nodeid="5596"><img src="https://s0.lgstatic.com/i/image/M00/3D/00/CgqCHl8o-amAegD7AAxcPQ_L2p0435.png" alt="Drawing 14.png" data-nodeid="5759"></p>
<p data-nodeid="5597">在这些海报实验的基础之上，他们总结出以下论点：</p>
<ul data-nodeid="5598">
<li data-nodeid="5599">
<p data-nodeid="5600">出现面部表情，更容易引人入胜；</p>
</li>
<li data-nodeid="5601">
<p data-nodeid="5602">使用反派形象更容易点击；</p>
</li>
<li data-nodeid="5603">
<p data-nodeid="5604">海报人数不要超过 3 人；</p>
</li>
<li data-nodeid="5605">
<p data-nodeid="5606">同一个海报，在不同国家的偏好完全不一样。</p>
</li>
</ul>
<h4 data-nodeid="5607">其他案例</h4>
<ul data-nodeid="5608">
<li data-nodeid="5609">
<p data-nodeid="5610">墨迹天气，如下图所示。</p>
</li>
</ul>
<p data-nodeid="5611"><img src="https://s0.lgstatic.com/i/image/M00/3C/F5/Ciqc1F8o-f6Ac06kAAJm-4H89vg428.png" alt="Drawing 16.png" data-nodeid="5769"></p>
<p data-nodeid="5612"><img src="https://s0.lgstatic.com/i/image/M00/3D/00/CgqCHl8o-hyAdlWPAABTOVU4nj0209.png" alt="Drawing 18.png" data-nodeid="5772"><br>
大家会看到右上角是一个分享按钮，我第一次看到这个按钮时就觉得很奇怪，因为在我的印象中，大部分按钮是小三角形少了一条边的那种形状。而墨迹天气，它竟然是这样一个标志，它最后采用的一定是 A/B 测试后的一个效果，这个确实要比那种形状要好。</p>
<ul data-nodeid="5613">
<li data-nodeid="5614">
<p data-nodeid="5615">滴滴，如下图所示。</p>
</li>
</ul>
<p data-nodeid="5616"><img src="https://s0.lgstatic.com/i/image/M00/3D/00/CgqCHl8o-iiABAZjAAD4Jjd4aCs700.png" alt="Drawing 19.png" data-nodeid="5778"></p>
<p data-nodeid="5617">当你打开进去时，你会发现头像那里实际上是有一个小汽车的图片。他们肯定也是做了同样的 A/B 测试，通过测试得出有图片的用户体验确实要比没图片的好。</p>
<ul data-nodeid="5618">
<li data-nodeid="5619">
<p data-nodeid="5620">今日头条里面的广告位，如下图所示。</p>
</li>
</ul>
<p data-nodeid="5621"><img src="https://s0.lgstatic.com/i/image/M00/3D/00/CgqCHl8o-jKAF4cIAAGzYEFoJpo366.png" alt="Drawing 21.png" data-nodeid="5783"></p>
<p data-nodeid="5622">这种广告位放在第几位效果比较好呢？这里是放在第 4 位，其实这种也是通过 A/B 测试得来的。理论上肯定是越往前越好，但是你这个时候越往前，可能用户的体验感越差。</p>
<p data-nodeid="5623">这些案例实际上都是商业化和用户体验的平衡，拿不准的时候就要去做 A/B 测试，而不是说靠感觉。</p>
<h4 data-nodeid="5624">案例思考</h4>
<p data-nodeid="5625">最后就是我的一些思考， A/B 测试会涉及三方：设计师、产品经理、数据分析师。</p>
<p data-nodeid="5626">对于设计师来说，设计师一定要跳出传统的设计思维，在这个基础之上，要多去看一些 A/B 测试增长黑客的书。在做设计时，你要把你所设计的任何细节（比如界面的优化）跟最终的 A/B 测试挂钩，然后看功能效果。设计师本身并不是设计完就不管了，你自己也要去看数据，这样无论是在效率上还是效果上都会有很好的提升。</p>
<p data-nodeid="5627">对于产品经理来说，光凭直觉是不靠谱的，A/B 测试的闭环能够让我们去更好地理解用户。同时要通过 A/B 测试总结出用户到底喜欢什么样的策略和界面，然后让 A/B 测试自身完成自我迭代。</p>
<p data-nodeid="5628">对于数据分析师来说，大多数改动都不会带来大福效果的提升，A/B 测试的效果往往都是略好，所以要持续迭代。如果某一个 A/B 测试，它的实验效果非常好，比如说实验组比对照组它的 CTR 提升了 30%，这时候分析师就要非常小心，是不是实验本身有问题。分析师在做这件事时，它是一个非常漫长的过程。 同时专题分析也是一个持续的过程，一定要越来越深入。当你通过 A/B 测试得出了一些结论，这表示你已经越来越了解用户，越来越了解产品。那么当你再做专题分析时，它的质量就越来越高。</p>
<p data-nodeid="5629">最后请举出一个你身边的 A/B 测试例子，可以是你参与过的，又或者是你听同事说的，说出你的感受和疑问，在评论区留言。</p>
<p data-nodeid="5630">今天的课程就到这里，欢迎你关注我本人的公众号（微信搜索：数据分析学习之道），之后会定期更新原创高质量的数据分析文章，下节课见，谢谢。</p>
<p data-nodeid="5631" class="te-preview-highlight"><a href="https://wj.qq.com/s2/6894820/1708/" data-nodeid="5795">这是课程评价链接，快来帮花木老师评价下吧！</a></p>

---

### 精选评论

##### **斌：
> 同问，ab测试的时候，如何做到选择相同特征的人群，这个工作是设计师负责，还是数据分析师呢？

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 算法工程师负责

##### **果：
> 请问老师如何保证几组实验的人群特征是一样的呢，比如如何保证他们的活跃程度一样

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; AB测试在设计的时候就已经考虑了两组一样，这里面是基于统计学

##### **艺：
> 想问老师 如何量化的评价A/B测试的效果 究竟是依据哪些指标看出哪个案例的效果更好

 ###### &nbsp;&nbsp;&nbsp; 编辑回复：
> &nbsp;&nbsp;&nbsp; 根据A/B两者中不一样的那个变量

##### **玲：
> 设计哪些指标是一样主要是看业务对吗？因为很多情况不能做到一样

 ###### &nbsp;&nbsp;&nbsp; 编辑回复：
> &nbsp;&nbsp;&nbsp; 是，根据业务来。

##### **华：
> 去师推荐下业界的AB测试平台

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 现阶段有很多AB测试平台，Google搜索一下就会出来很多哈，至于异同点可以根据你业务去筛选。

##### **萍：
> 第19讲简洁版思维导图已更新 请自取 谢谢https://github.com/Amberjay18/DataAnalysisThinkingMindMap/tree/master/Mind%20Map%20Concise%20Version/19

##### **萍：
> 第19讲的思维导图已经更新 请自取 谢谢https://github.com/Amberjay18/DataAnalysisThinkingMindMap/tree/master/DataAnalysisThinkingMindMap_19

##### **萍：
> 干货满满 谢谢花木老师 学到了更详尽的A/B测试内容😁

##### **源：
> 产品曾经做的不严谨“A/B测试”：1. 改动后和改动前比较2. 直接比较没改动和改动的用户（未做筛选）这种情况下苦的还是提数的

