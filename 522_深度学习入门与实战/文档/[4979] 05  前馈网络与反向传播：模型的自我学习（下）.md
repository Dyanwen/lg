<p data-nodeid="103733" class="">在之前的课程中，咱们先学习了深度学习会用到的数学知识和基础原理，随后又学习了模型的目标函数、优化原理基础。关于深度学习的内部原理基本上已经万事俱备，就差最后一个环节：<strong data-nodeid="103739">模型学习的计算过程具体是怎样的</strong>？本节课咱们就结合数学公式来学习这个过程。</p>



<p data-nodeid="101570">模型的学习本质就是数学的计算过程，因此，本章节会涉及很多的运算，我会尽量简化运算的过程。</p>
<h3 data-nodeid="101571">前馈网络</h3>
<p data-nodeid="106604">前馈神经网络（Feedforward Neural Network，FNN），简称前馈网络，它是一种单向的多层结构，也是最简单的神经网络，其简化结构图如下所示：</p>
<p data-nodeid="106605" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DA/CgqCHl-iXxSACUeBAAQf-THfqtE403.png" alt="Drawing 0.png" data-nodeid="106609"></p>


<p data-nodeid="101574">在这个网络中，蓝色的层是第 0 层，我们称为<strong data-nodeid="101747">输入层</strong>，这里将接受模型的输入数据，即向量 x；中间黄色的层，分别是有 5 个神经元的第 1 层和有 3 个神经元的第 2 层，它们是模型的内部环节，我们称为<strong data-nodeid="101748">隐藏层</strong>（实际的网络中，隐藏层可以有很多层，不仅限于 2 个）；绿色的层，是网络的最后一层，我们称为<strong data-nodeid="101749">输出层</strong>。神经元之间的单向箭头连线，就是两个节点之间的权重。</p>
<p data-nodeid="101575">既然叫前馈神经网络，那我们理解了“前馈”就相当于理解了这个网络。前馈是指这个网络中的每一层神经元，产生信号之后会传递到下一层，而下一层的神经元产生的信号无法反传递给上一层，即<strong data-nodeid="101755">数据是单向流动的</strong>。</p>
<p data-nodeid="109474">那输入的数据最后是怎么变成输出的呢？为了方便起见，咱们把上图进一步简化，来看看它的具体计算过程。</p>
<p data-nodeid="109475" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CE/Ciqc1F-iXxyAMWXFAALPqKe7PeU168.png" alt="Drawing 1.png" data-nodeid="109479"></p>


<p data-nodeid="101578">这个简单的网络中，有一个输入层（a、b 节点），一个隐藏层（c、d 节点）和一个输出层（e 节点）。</p>
<p data-nodeid="101579">在开始具体计算之前，咱们先明确几个变量。</p>
<ul data-nodeid="112334">
<li data-nodeid="112335">
<p data-nodeid="112336"><strong data-nodeid="112342">输入数据 x</strong>：</p>
</li>
</ul>
<p data-nodeid="112337" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DA/CgqCHl-iXySAcrz7AAAqwPVhS20331.png" alt="Drawing 3.png" data-nodeid="112345"></p>



<ul data-nodeid="115184">
<li data-nodeid="115185">
<p data-nodeid="115186"><strong data-nodeid="115209">权重 W</strong><sub>ij</sub>，表示从节点 i 和 j 之间的权重，方向由 j 到 i。在上图中，我们假设 W<sub>cb</sub>=0.7，W<sub>da</sub>=0.3，W<sub>ec</sub>=0.4。</p>
</li>
<li data-nodeid="115187">
<p data-nodeid="115188"><strong data-nodeid="115225">权重矩阵 W</strong><sub>j</sub>，表示第 j 层的权重矩阵，在这里有两个权重矩阵，W<sub>1</sub> 和 W<sub>2</sub>：</p>
</li>
</ul>
<p data-nodeid="115189" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DA/CgqCHl-iXyyAH49gAADcDSIMPzg060.png" alt="Drawing 5.png" data-nodeid="115228"></p>



<ul data-nodeid="118053">
<li data-nodeid="118054">
<p data-nodeid="118055"><strong data-nodeid="118061">激活函数</strong>。这节课我选择 Sigmoid 函数作为激活函数，其公式化表示为：</p>
</li>
</ul>
<p data-nodeid="118056" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iXzKAbMEAAAAtK0qxvEI201.png" alt="Drawing 7.png" data-nodeid="118064"></p>



<blockquote data-nodeid="101597">
<p data-nodeid="101598">激活函数实际上是一个将线性计算过程进行非线性变换的函数。关于激活函数我会在讲到介绍深度学习的各种运算单元和结构时进行详细介绍，目前咱们可以单纯地理解为：激活函数是一个对输入数据进行变化的函数。</p>
</blockquote>
<p data-nodeid="101599">明确这些变量之后，我们就可以开始具体的计算了。</p>
<p data-nodeid="120873">节点 c 的输入，就是 x<sub>1</sub> 和 x<sub>2</sub> 沿着各自的路径与对应的权重相乘后求和，即：</p>
<p data-nodeid="120874" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iXzyAFnZxAABiySo0LfM351.png" alt="Drawing 9.png" data-nodeid="120886"></p>



<p data-nodeid="123679">同理，节点 d 的输入就是：</p>
<p data-nodeid="123680" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DA/CgqCHl-iX0OAGkOlAABirHQMfXI199.png" alt="Drawing 11.png" data-nodeid="123684"></p>



<p data-nodeid="126461">模型在计算的过程中是使用矩阵的方式，所以得到的结果也是以层为单位的，计算过程如下：</p>
<p data-nodeid="126462" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DA/CgqCHl-iX0qAJpJ6AACYA7Fo9Dg060.png" alt="Drawing 13.png" data-nodeid="126466"></p>



<blockquote data-nodeid="101609">
<p data-nodeid="101610">如果你对矩阵的运算有点忘记了，可以去《01｜从神经元说起：数学篇》回顾。</p>
</blockquote>
<p data-nodeid="129227">现在，输入的数据走到了 c 和 d 两个节点，这里还有一层Sigmoid激活函数。因此，第一层的输出是：</p>
<p data-nodeid="129228" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DA/CgqCHl-iX1SAHcYIAACAEqWYilo260.png" alt="Drawing 15.png" data-nodeid="129232"></p>



<p data-nodeid="101614">也就是节点 c 的输出为 0.6479，节点 d 的输出为 0.5769。</p>
<p data-nodeid="131977">这样，咱们就完成了隐藏层的计算了。咱们可以用同样的方式求到最后节点 e 的输出，过程如下：</p>
<p data-nodeid="131978" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iX12AFT9DAAB_jnSmxQk051.png" alt="Drawing 17.png" data-nodeid="131982"></p>



<p data-nodeid="101618">输入的数据 x 经过网络的各个节点之后，最后模型给出计算结果为 0.6469。到这里，我们就完成了一个最基本的前馈网络从输入到输出的计算过程。</p>
<h3 data-nodeid="101619">链式法则</h3>
<p data-nodeid="101620">深度学习的内容主要就是优化更新各个节点之间链接的权重，也就是刚才提到的 W。</p>
<p data-nodeid="101621">那如何更新权重呢？我在上一课时提到“<strong data-nodeid="101881">模型的学习就是不断减小损失函数</strong>”。最小化损失函数通常采用梯度下降的方式，即每一次给模型更新权重的时候按照梯度的方向进行。</p>
<p data-nodeid="134711">假设我们把 cost 函数表示为：H(W<sub>11</sub>,W<sub>12</sub>,...,W<sub>ij</sub>...,W<sub>mn</sub>)，则其梯度向量▽H为：</p>
<p data-nodeid="134712" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DA/CgqCHl-iX2iAfVqaAAB74gmfE7Q295.png" alt="Drawing 19.png" data-nodeid="134732"></p>



<p data-nodeid="101625">是否发现了一个问题？就是上面梯度向量的一些项不太好求。比如第一项，W<sub>11</sub> 似乎跟 H 的关联不是很直接，因为中间隔了很多的节点和层。这个时候我们就需要微积分中的链式法则出场了。</p>
<p data-nodeid="137445">所谓链式法则，是指<strong data-nodeid="137452">两个函数组合起来的复合函数，其导数等于里面函数代入外函数值的导数，乘以里面函数的导数</strong>。假设有函数 f(g(x))，链式法则有两种形式：</p>
<p data-nodeid="137446" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DA/CgqCHl-iX3CAE17VAACCaKUbxo0838.png" alt="Drawing 21.png" data-nodeid="137455"></p>



<p data-nodeid="101629">链式法则的定义看起来很拗口，没关系，咱们以第一种形式举个例子。</p>
<p data-nodeid="101630">有函数 f(x)=sin(x<sup>3</sup> + 5)。我们可以把函数分解为：</p>
<ol data-nodeid="101631">
<li data-nodeid="101632">
<p data-nodeid="101633">f(x)=sin(x)；</p>
</li>
<li data-nodeid="101634">
<p data-nodeid="101635">g(x)=x<sup>3</sup> + 5。</p>
</li>
</ol>
<p data-nodeid="101636">g(x)的导数为 g'(x) = 3x<sup>2</sup>，f(x)的导数为 f'(x)=cos(x)，则 f'(x) = f'(g(x))g'(x)=cos(x<sup>3</sup> + 5)·3x<sup>2</sup>，相当于各自求导后再相乘。</p>
<p data-nodeid="140152">在求梯度之前，咱们把链式法则的例子变得稍微复杂一点，通过一个例子来感受链式法则的具体过程。下图是一个 y=(a+b)(b*c)函数的关系图，运算方向为自下向上。</p>
<p data-nodeid="140153" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iX32AAWZjAAE6-GTQLn4388.png" alt="Drawing 23.png" data-nodeid="140159"></p>



<p data-nodeid="142840">要想求函数 y 在 a=1，b=2，c=3 条件下的梯度，我们可以先利用偏导数的定义求出不同层之间相邻节点的偏导关系。我将每个导数的计算写在了图中，如下：</p>
<p data-nodeid="142841" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iX4SAOJGcAAHKozyNw_U060.png" alt="Drawing 25.png" data-nodeid="142845"></p>



<p data-nodeid="145518">通过链式法则我们知道：</p>
<p data-nodeid="145519" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DA/CgqCHl-iX4yAZBR2AADmEEB29VA908.png" alt="Drawing 26.png" data-nodeid="145523"></p>


<p data-nodeid="101645">可以看出，函数 F 在 a 的偏导数相当于从 a 到 F 路径上的偏导值的乘积，同理 F 在 b 的偏导数等于 b 到 F 的所有路径上偏导值乘积的和。是不是很简单？很好，咱们已经把链式法则基本弄明白了。可以开始探讨反向传播的过程了。</p>
<h3 data-nodeid="101646">反向传播</h3>
<p data-nodeid="101647">反向传播算法（Backpropagation）是目前训练神经网络最常用且最有效的算法。通过反向传播，模型会不断更新自己的参数，以达到学习的目的。在这个过程中会经历 3 个步骤。</p>
<ol data-nodeid="101648">
<li data-nodeid="101649">
<p data-nodeid="101650"><strong data-nodeid="101984">前向传播</strong>：将训练数据输入到网络中，数据经过隐藏层，最后达到输出层并作为结果输出。</p>
</li>
<li data-nodeid="101651">
<p data-nodeid="101652"><strong data-nodeid="101989">误差及其传播</strong>：计算输出值和实际值之间的误差，将误差从输出层向隐藏层反向传递，直到输入层。</p>
</li>
<li data-nodeid="101653">
<p data-nodeid="101654"><strong data-nodeid="101994">迭代</strong>：在反向传播的过程中，根据误差调整模型各个参数的值，并不断迭代前两个步骤，直至达到结束模型训练的条件。</p>
</li>
</ol>
<blockquote data-nodeid="101655">
<p data-nodeid="101656">反向传播的公式推导和证明是相对冗长和复杂的，在实际的深度学习的研发中，该部分的内容都是封装好的，所以咱们这里并不会对反向传播的数学定义、推导、证明进行详细的介绍。但是我们需要知道和了解这个过程的具体运转方式，这将有助于我们更好地理解深度学习是如何学习的。</p>
</blockquote>
<p data-nodeid="101657">在开始下面的内容之前，你不妨重新看一下《<strong data-nodeid="102001">04｜函数与优化方法：模型的自我学习（上）</strong>》这一课时中关于梯度下降的内容。下面我要讲的就是将相关知识进行实际举例的过程。</p>
<p data-nodeid="148196">我们再回到前馈网络中使用到的简化网络：</p>
<p data-nodeid="148197" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iX5uAQSNJAAJeIdkuiWE134.png" alt="Drawing 27.png" data-nodeid="148201"></p>


<p data-nodeid="101660">在前面的内容中，我们已经学会如何得到这个网络中所有节点和边（权重）的数值了，其中</p>
<p data-nodeid="101661">输入为：x<sub>1</sub>=0.5，x<sub>2</sub>=0.8。</p>
<p data-nodeid="101662">第一层权重：W<sub>ca</sub>=0.1，W<sub>cb</sub>=0.7，W<sub>da</sub>=0.3，W<sub>db</sub>=0.2</p>
<p data-nodeid="101663">节点 c 输入：h(c)=W<sub>ca</sub>*x<sub>1</sub>+W<sub>cb</sub>*x<sub>2</sub>=0.61，节点 d 输入：h(d)=W<sub>da</sub>*x<sub>1</sub>+W<sub>db</sub>*x<sub>2</sub>=0.31</p>
<p data-nodeid="101664">节点 c 输出：O(c)=0.647941，节点 d 输出：O(d)=0.576885</p>
<p data-nodeid="101665">第二层权重：W<sub>ec</sub>=0.4，W<sub>ed</sub>=0.6</p>
<p data-nodeid="101666">节点 e 输入：h(e)=W<sub>ec</sub>*Output(c)+W<sub>ed</sub>*Output(d)=0.605307</p>
<p data-nodeid="101667">节点 e 输出：O(e)=0.646870</p>
<p data-nodeid="101668">实际标签：y=1</p>
<p data-nodeid="150866">损失函数为：</p>
<p data-nodeid="150867" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iX66ACFY0AABxkGYYBh8247.png" alt="Drawing 29.png" data-nodeid="150871"></p>



<p data-nodeid="153520">激活函数为：</p>
<p data-nodeid="153521" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iX7iALgV8AAAwikAgIh4370.png" alt="Drawing 31.png" data-nodeid="153525"></p>



<p data-nodeid="101675">咱们用这个简化的网络，一步步看反向传播是怎么进行的。</p>
<p data-nodeid="101676">我在“<strong data-nodeid="102113">04 课时</strong>”讲过，咱们使用损失函数来评估拟合函数对真实函数的表示效果的“好坏”，为了更新权重，就需要求损失函数的梯度，即 L(x)对不同权重的偏导数。</p>
<p data-nodeid="101677">反向传播的特点是<strong data-nodeid="102131">反馈从输出往输入方向流动</strong>，所以最先要更新的权重是最后一个隐藏层的权重，即 W<sub>ec</sub>和W<sub>ed</sub>。我们以更新 W<sub>ec</sub> 为例，具体过程分为 4 个步骤。</p>
<p data-nodeid="180283" class="">1.<strong data-nodeid="180343">确定权重到输出的路径</strong>。很显然，数据是从节点 c 流动到 e 然后输出的。<br>
2.<strong data-nodeid="180344">确定路径上的函数关系</strong>。首先由 O(c)和权重 W<sub>ec</sub> 相乘得到节点 e 的一部分输入，另一部分则是 O(d)和权重 W<sub>ed</sub> 相乘得到的。涉及的函数为 h=W<sub>ec</sub>*O(c)+W<sub>ed</sub>*O(d)，e 节点还有一个激活函数 f(x)。所以最后的函数关系为：<br>
1. L=1/2 * (O(e) - y)^2<br>
2. O(e) = f(h)<br>
3. h = W<sub>ec</sub>*O(c)+W<sub>ed</sub>*O(d)<br>
3.<strong data-nodeid="180345">计算损失函数对 W</strong><sub>ec</sub><strong data-nodeid="180346">的偏导</strong>。</p>





<p data-nodeid="156170" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DA/CgqCHl-iX8mAG39aAACyZzUFJZY511.png" alt="Drawing 33.png" data-nodeid="156228"></p>



<p data-nodeid="174299" class="">4.<strong data-nodeid="174309">更新权重</strong>。该环节就是之前咱们介绍梯度下降的时候介绍过的环节：更新参数。假定我们的学习率α=0.1，则W<sub>ec</sub> 的更新值为：</p>


<p data-nodeid="158848" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iX9CAJZl4AAEfOKHF5-Y517.png" alt="Drawing 35.png" data-nodeid="158860"></p>



<p data-nodeid="101699">由此，我们完成了 W<sub>ec</sub> 的权重更新。同理，我们按照这个步骤更新其他的五个权重。</p>
<p data-nodeid="161461">W<sub>ed</sub>，路径关系为节点 d 到节点 e。更新过程如下：</p>
<p data-nodeid="161462" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iX9iAN8OlAAGIzI_BWas147.png" alt="Drawing 37.png" data-nodeid="161470"></p>



<p data-nodeid="164055">W<sub>ca</sub>，路径关系为节点 a 到节点 c 到节点 e。更新过程如下：</p>
<p data-nodeid="164056" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iX-GAKpJ1AAFY7lwym8I721.png" alt="Drawing 39.png" data-nodeid="164064"></p>



<p data-nodeid="166633">W<sub>cb</sub>，路径关系为节点 b 到节点 c 到节点 e。更新过程如下：</p>
<p data-nodeid="166634" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DB/CgqCHl-iX-mAOOe7AAF25dp2gmA143.png" alt="Drawing 41.png" data-nodeid="166642"></p>



<p data-nodeid="169195">W<sub>da</sub>，路径关系为节点 a 到节点 d 到节点 e。更新过程如下：</p>
<p data-nodeid="169196" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/DB/CgqCHl-iX_SABKNrAAGg9d79eyY085.png" alt="Drawing 43.png" data-nodeid="169204"></p>



<p data-nodeid="171741">W<sub>db</sub>，路径关系为节点 b 到节点 d 到节点 e。更新过程如下：</p>
<p data-nodeid="171742" class=""><img src="https://s0.lgstatic.com/i/image/M00/67/CF/Ciqc1F-iX_yAcwztAAGFu4C1N8E748.png" alt="Drawing 45.png" data-nodeid="171750"></p>



<p data-nodeid="101715">为了验证模型是否真的学到了知识，我们不妨重新使用 x<sub>1</sub> 和 x<sub>2</sub> 在更新权重后的模型进行预测。按照之前的前馈计算步骤可以得到输出 O(e)=0.648366218，对应的损失函数 L(x)=0.061823158。对比原来的 L(x)=0.062351，我们可以发现损失函数变小了。如果有更多数据进行更多的迭代，这个损失函数会越来越小，模型的表现也会越来越好。</p>
<p data-nodeid="101716">以上就是反向传播的基本过程。细心的你一定会发现一个问题，那就是在进行梯度更新的时候，很多变量被反复计算了，比如 O<sub>e</sub>、h<sub>e</sub>，如果每次更新梯度都要重新计算，那整个的运算量就很大，所以在实际的反向传播算法中，每个节点都会暂存从上层反馈回来的各个数据值信息，也就避免了重复计算，提高运算速度。</p>
<h3 data-nodeid="101717">总结</h3>
<p data-nodeid="101718">本课时我对前馈网络和反向传播算法进行了具体的解释和计算，在实际工作中这部分的内容是封装好的。尽管如此，我们仍旧需要了解其原理及过程。自此，通过前 5 个课时的内容，我介绍了深度学习相关的数学知识和原理，下一课时我将会通过一个具体的问题把所有的知识点串联起来，为后续进一步介绍深度学习的结构、网络和实战做好准备。</p>
<p data-nodeid="101719">关于本章节，关于反向传播你还有什么疑问，欢迎留言。</p>

---

### 精选评论

##### *波：
> 多变量求偏导的时候，比如f = xy，对x求偏导就把y看作常量，这里x是一元，所以最终偏导等于1 * y = y。f = x + y 对x求偏导时忽略常量y，x是一元的，最终结果就是1。

##### **丽：
> O(e)对h(e)求导是怎么得到h(e)*(1-h(e))？

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 好好看一下函数的导数呀。

##### **升：
> 写的非常好！以前看完 NG的视频再来看这个理解更好了

##### **威：
> 老师，我想问一下，模型里面初始的权重是从哪里来的呢

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 在你声明cnn或者全联接等结构的时候，就会自动生成，而且也可以使用不同的方式声明，比如正态分布、平均分布等。

##### **6060：
> 老师你好， 我问一个题话外， 我知道今年腾讯有个预测广告受众基础属性预估的比赛， 想问下，学多久有能力参加这样的比赛？ 学多久能够拿到比较不错的名次呢？

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; Kaggle有类似的比赛，你可以到上面尝试着做一下，横向比较自己跟其他队伍的分数，心里就有个数了。

##### **烁：
> 上边讲的反向传播讲的是以一个数据为例，进行的一个梯度下降，当数据集越大就执行上述操作更多次。我不知道这么理解对不对？

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 对滴，数据越多，学习/更新就越多，一直到满足结束条件后停止。

