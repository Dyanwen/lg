<p data-nodeid="2517" class="">你好，我是槐树，欢迎来到深度学习的 01 课时。从今天开始，我将与你一起，开始一场特殊的旅行：从零开始学习人工智能。</p>
<p data-nodeid="2518">我们先来看本课程的第一节课，<strong data-nodeid="2664">01 | 从神经元说起：数学篇</strong>。说到深度学习，很多人都以为它很深奥、很难，需要很多数学知识。其实并不是这样的，本科的数学知识已经足够你掌握中等水平的深度学习了。我也会在这一节带你了解后续课程中会用到的，深度学习中的基础数学知识。</p>
<p data-nodeid="2519">这一节，除了介绍数学计算规则之外，我还会使用 Python 程序代码的方式进行实际演示，这可以让你更加直观地了解你学习的知识是如何在实际中应用的。我会从使用最多的，也是较为重要的线性代数、微积分和信息论这 3 个部分出发，带你了解深度学习中的数学部分。</p>
<h3 data-nodeid="2520">线性代数部分知识点回顾</h3>
<p data-nodeid="2521">深度学习背后的核心是标量、向量、矩阵和张量这 4&nbsp;种数据结构，我们可以通过使用这些数据结构，以编程的方式解决所有基本的线性代数问题。我会在后续介绍 TensorFlow 的时候再对张量进行介绍。除此之外，我增加了范数的概念，这个知识点在后续的函数优化方面有着很重要的作用。</p>
<p data-nodeid="2522">我们先来了解一下标量和向量。</p>
<h4 data-nodeid="2523">1. 标量</h4>
<p data-nodeid="2524">标量，实际上就是一个单独的数。代码如下：</p>
<pre class="lang-dart" data-nodeid="2525"><code data-language="dart">a = <span class="hljs-number">1.0</span>
<span class="hljs-built_in">print</span>(type(a))
# Get &lt;<span class="hljs-class"><span class="hljs-keyword">class</span> '<span class="hljs-title">float</span>'&gt;
<span class="hljs-title">b</span> = 5
<span class="hljs-title">print</span>(<span class="hljs-title">type</span>(<span class="hljs-title">b</span>))
# <span class="hljs-title">Get</span> &lt;<span class="hljs-title">class</span> '<span class="hljs-title">int</span>'&gt;
</span></code></pre>
<h4 data-nodeid="2526">2. 向量及其运算</h4>
<p data-nodeid="2527">一个向量表示一组有序排列，并可以通过索引获取其中对应位置的数值。一般情况下，我们会选择 NumPy 对向量进行表示和计算。NumPy 是 Python 的一个扩展程序库，能够很好地支持数组、向量、矩阵的运算。它的官网地址为：<a href="https://numpy.org/" data-nodeid="2679">https://numpy.org/</a>。</p>
<p data-nodeid="2528">我们来看一下向量是如何基于 NumPy 运算的。</p>
<p data-nodeid="2529"><strong data-nodeid="2685">2.1 向量和标量的计算</strong></p>
<p data-nodeid="2530">我刚才讲到，标量是一个数字，所以标量在跟向量进行加减乘除的时候，实际上与向量中的每一个数字都同步进行了计算。代码如下：</p>
<pre class="lang-dart" data-nodeid="2531"><code data-language="dart"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
a=np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
<span class="hljs-built_in">print</span>(a * <span class="hljs-number">4</span>)
# Get&nbsp;array([<span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">12</span>, <span class="hljs-number">16</span>])
<span class="hljs-built_in">print</span>(a + <span class="hljs-number">4</span>)
# Get&nbsp;array([<span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>])
</code></pre>
<p data-nodeid="2532"><strong data-nodeid="2690">2.2 向量之间的加减操作</strong></p>
<p data-nodeid="2533">向量之间的加减操作是各自对应位置的加减操作。因为 Python 的数组相加是列表的拼接操作，所以在 Python 中，向量的计算不能使用 Python 数组的计算方法。</p>
<p data-nodeid="2534">例如，给定如下图的两个向量，我们经过加操作之后，就得到了如下的结果。</p>
<p data-nodeid="2535"><img src="https://s0.lgstatic.com/i/image/M00/61/92/CgqCHl-P7-6AM-N2AACU7YNmeQU223.png" alt="Drawing 0.png" data-nodeid="2695"></p>
<p data-nodeid="2536">代码如下：</p>
<pre class="lang-dart" data-nodeid="2537"><code data-language="dart">x = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]
y = [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]
<span class="hljs-built_in">print</span>(x + y)
# Get:&nbsp;[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-built_in">print</span>(np.add(x, y))
# Get:&nbsp;array([<span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>])
</code></pre>
<p data-nodeid="2538"><strong data-nodeid="2700">2.3 向量之间的乘法操作</strong></p>
<p data-nodeid="2539">向量之间的乘法操作主要分为点乘（内积）、叉乘（外积）和对应项相乘。</p>
<p data-nodeid="2540"><strong data-nodeid="2706">我们先来看向量的点乘</strong>。</p>
<p data-nodeid="2541">向量的点乘，也叫向量的内积、数量积，对两个向量执行点乘运算，就是对这两个向量对应位一一相乘之后求和的操作，点乘的结果是一个标量。给定两个向量：a=[a<sub>1</sub>, a<sub>2</sub>...a<sub>n</sub>]和 b=[b<sub>1</sub>,b<sub>2</sub>...b<sub>n</sub>]，则 a 和 b 的点乘计算方式为 a·b = a<sub>1</sub>b<sub>1</sub>+a<sub>2</sub>b<sub>2</sub>+....+a<sub>n</sub>b<sub>n</sub>。</p>
<p data-nodeid="2542"><strong data-nodeid="2766">向量的点乘要求两个向量的长度一致</strong>。</p>
<p data-nodeid="2543"><strong data-nodeid="2771">其次是向量的叉乘</strong>。</p>
<p data-nodeid="2544">向量的叉乘，也叫向量的外积、向量积。叉乘的运算结果是一个向量而不是一个标量。叉乘用得较少，这里就不多介绍。</p>
<p data-nodeid="2545"><strong data-nodeid="2777">最后是对应项相乘</strong>。</p>
<p data-nodeid="2546">对应项相乘，顾名思义，就是两个向量对应的位置相乘，得到的结果还是原来的形状。给定两个向量：a=[a<sub>1</sub>, a<sub>2</sub>...a<sub>n</sub>]和b=[b<sub>1</sub>,b<sub>2</sub>...b<sub>n</sub>]，则 a 和&nbsp;b&nbsp;的对应项相乘计算方式为a*b = [a<sub>1</sub>b<sub>1</sub>,a<sub>2</sub>b<sub>2</sub>...a<sub>n</sub>b<sub>n</sub>]。</p>
<p data-nodeid="2547">点乘、叉乘及对应项相乘的代码如下所示：</p>
<pre class="lang-dart" data-nodeid="3146"><code data-language="dart"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
a=np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])
b=np.array([<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>])
<span class="hljs-built_in">print</span>(np.inner(a, b))
# 点乘：Get <span class="hljs-number">70</span>
<span class="hljs-built_in">print</span>(np.outer(a, b))
# 叉乘：Get 
    array([[ <span class="hljs-number">5</span>,&nbsp; <span class="hljs-number">6</span>,&nbsp; <span class="hljs-number">7</span>,&nbsp; <span class="hljs-number">8</span>],
&nbsp; &nbsp; &nbsp; &nbsp;[<span class="hljs-number">10</span>, <span class="hljs-number">12</span>, <span class="hljs-number">14</span>, <span class="hljs-number">16</span>],
&nbsp; &nbsp; &nbsp; &nbsp;[<span class="hljs-number">15</span>, <span class="hljs-number">18</span>, <span class="hljs-number">21</span>, <span class="hljs-number">24</span>],
&nbsp; &nbsp; &nbsp; &nbsp;[<span class="hljs-number">20</span>, <span class="hljs-number">24</span>, <span class="hljs-number">28</span>, <span class="hljs-number">32</span>]])
<span class="hljs-built_in">print</span>(np.multiply(a,b))
# 对应项相乘：Get&nbsp;array([ <span class="hljs-number">5</span>, <span class="hljs-number">12</span>, <span class="hljs-number">21</span>, <span class="hljs-number">32</span>])
</code></pre>
<h4 data-nodeid="3147">3. 矩阵及其运算</h4>
<p data-nodeid="3148">介绍完标量和向量之后，我们再来看矩阵。</p>
<p data-nodeid="3149">矩阵一般是一个 m 行 n 列的矩形阵列，一般的表达方式如下图所示：</p>
<p data-nodeid="3150"><img src="https://s0.lgstatic.com/i/image/M00/61/87/Ciqc1F-P8B2AC68KAAFm8hNYuMM471.png" alt="Drawing 1.png" data-nodeid="3261"></p>
<p data-nodeid="3151">矩阵中每个元素都有 m 和 n 两个下标，分别代表行和列的位置，所以矩阵也可以通过索引直接定位元素的值，例如 X<sub>[2][1]</sub>。不难发现，m=1 的时候，矩阵就成了刚才提到的向量。</p>
<p data-nodeid="3152"><strong data-nodeid="3275">3.1 矩阵的加减法</strong></p>
<p data-nodeid="3153">矩阵的加减法操作跟向量类似，也是对应位置进行相加减。如图所示，红色和绿色的框分别代表了不同位置数字的计算过程。</p>
<p data-nodeid="3154"><img src="https://s0.lgstatic.com/i/image/M00/61/87/Ciqc1F-P8COAPVriAAD-vfCB2oY521.png" alt="Drawing 2.png" data-nodeid="3279"></p>
<p data-nodeid="3155">代码如下所示：</p>
<pre class="lang-dart" data-nodeid="3156"><code data-language="dart"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
a = np.mat(((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),(<span class="hljs-number">5</span>,<span class="hljs-number">6</span>)))
b = np.mat(((<span class="hljs-number">0</span>,<span class="hljs-number">1</span>),(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)))
<span class="hljs-built_in">print</span>(a)
# Get matrix([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])
<span class="hljs-built_in">print</span>(a+b)
# Get&nbsp;matrix([[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>],
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [<span class="hljs-number">7</span>, <span class="hljs-number">9</span>]])
</code></pre>
<p data-nodeid="3157"><strong data-nodeid="3284">3.2 矩阵的乘运算</strong></p>
<p data-nodeid="3158">矩阵的乘运算也有两种形式。</p>
<p data-nodeid="3159"><strong data-nodeid="3290">第一种是两个形状一样的矩阵的对应位置分别相乘</strong>。</p>
<p data-nodeid="3160"><img src="https://s0.lgstatic.com/i/image/M00/61/87/Ciqc1F-P8DCAaI42AADX-9vIAb4685.png" alt="Drawing 3.png" data-nodeid="3293"></p>
<p data-nodeid="3161"><strong data-nodeid="3298">第二种则是矩阵乘法</strong>。设 a 为 m 行 p 列的矩阵，b 为 p 行 n 列的矩阵，相乘的结果为一个 m 行 n 列的新矩阵，其中第 i 行第 j 列（1≤i≤m,1≤j≤n）的元素为：</p>
<p data-nodeid="3162"><img src="https://s0.lgstatic.com/i/image/M00/61/92/CgqCHl-P8DaAehS4AACYD-GFfSY258.png" alt="Drawing 4.png" data-nodeid="3301"></p>
<p data-nodeid="3163">具体放到示意图中，如下所示：</p>
<p data-nodeid="3164"><img src="https://s0.lgstatic.com/i/image/M00/61/92/CgqCHl-P8DyAdRX7AAGwRWoW1sY733.png" alt="Drawing 5.png" data-nodeid="3305"></p>
<p data-nodeid="3165">每个新的元素都是由一个行向量和一个列向量做点乘之后生成的。这个计算过程有点复杂。</p>
<pre class="lang-dart" data-nodeid="3166"><code data-language="dart"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
a = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
b = np.array([[<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>]])
<span class="hljs-built_in">print</span>(a*b)
# 对应位置相乘：Get&nbsp;array([[ <span class="hljs-number">5</span>, <span class="hljs-number">12</span>],
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [<span class="hljs-number">21</span>, <span class="hljs-number">32</span>]])
<span class="hljs-built_in">print</span>(a.dot(b))
# 矩阵乘法，Get&nbsp;array([[<span class="hljs-number">19</span>, <span class="hljs-number">22</span>],
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [<span class="hljs-number">43</span>, <span class="hljs-number">50</span>]])
</code></pre>
<h4 data-nodeid="3167">4. 范数</h4>
<p data-nodeid="3168">回顾完了标量、向量和矩阵，我们再来看看范数。<strong data-nodeid="3315">范数是一种距离的表示，或者说向量的长度</strong>。常见的范数有 L0 范数、L1 范数和 L2 范数，我们通过一个向量来看：</p>
<p data-nodeid="3169"><img src="https://s0.lgstatic.com/i/image/M00/61/92/CgqCHl-P8FiALX_4AADAG8duR4Y365.png" alt="Drawing 6.png" data-nodeid="3318"></p>
<p data-nodeid="3170"><strong data-nodeid="3322">4.1 L0 范数</strong></p>
<p data-nodeid="3171">L0 范数指这个向量中非 0 元素的个数。我们可以通过 L0 范数减少非 0 元素的个数，从而减少参与决策的特征，减少参数。这样一来，自然模型就运算得更快，模型的体积也更小了。</p>
<p data-nodeid="3172"><strong data-nodeid="3327">4.2 L1 范数</strong></p>
<p data-nodeid="3173">L1 范数指的是向量中所有元素的绝对值之和，它是一种距离的表示（曼哈顿距离），也被称为稀疏规则算子，公式如下：</p>
<p data-nodeid="3174"><img src="https://s0.lgstatic.com/i/image/M00/61/87/Ciqc1F-P8GOAQQNAAAC6vBe0PRQ440.png" alt="Drawing 7.png" data-nodeid="3331"></p>
<p data-nodeid="3175">L0 范数和 L1 范数都能实现权值稀疏。但 L1 范数是 L0 范数的最优凸近似，它比 L0 范数有着更好的优化求解的特性，所以被更广泛地使用。</p>
<p data-nodeid="3176">那么，<strong data-nodeid="3338">我们为什么要实现权值稀疏呢</strong>？</p>
<p data-nodeid="3177">在设计模型的过程中，我们有时会使用到大量的特征（例如在推荐系统中，特征维度都是上亿的），每个特征都会从不同的角度体现问题的不同信息。这些特征经过某些方式的组合、变换、映射之后，会按照不同的权重得到最终的结果。</p>
<p data-nodeid="3178">但有时候，有一部分特征对于最后结果的贡献非常小，甚至近乎零。这些用处不大的特征，我们希望能够将其舍弃，以更方便模型做出决策。这就是权值稀疏的意义。</p>
<p data-nodeid="3179"><strong data-nodeid="3344">4.3 L2 范数</strong></p>
<p data-nodeid="3180">除了 L0、L1，还有一个更为常用的范数 L2，也有叫它“岭回归”和“权值衰减”的。我们首先来看它的定义：L2 范数是向量中所有元素的平方和的平方根。其公式如下：</p>
<p data-nodeid="3181"><img src="https://s0.lgstatic.com/i/image/M00/61/92/CgqCHl-P8G2AA-o8AADJPW0zi7A429.png" alt="Drawing 8.png" data-nodeid="3348"></p>
<p data-nodeid="3182"><strong data-nodeid="3353">L2 也代表一种距离，即欧式距离</strong>。</p>
<p data-nodeid="3183">我们刚才提到 L0 和 L1 可以起到权值稀疏的作用，L2&nbsp;也有它的作用，那就是防止过拟合。我们先来看什么是过拟合，再来了解为什么 L2 可以减少过拟合。</p>
<p data-nodeid="3184">我先打一个比方。</p>
<p data-nodeid="3185">你在考试前一天临时抱佛脚，把去年的考试题、老师画的知识点，背得滚瓜烂熟。第二天考试的时候，卷子发下来，第一页的题都是你前一天背过的。你很开心，三下五除二就写好了，然后把卷子翻到了第二页。结果到了第二页你就傻眼了，都是你没背过的知识点，完全不知道它们在问什么，最后你挂科了，来年再见。</p>
<p data-nodeid="3186">咱们训练模型时，一般会有训练数据和测试数据。有时模型在训练数据上表现非常好，但一到测试数据上，效果就会急剧下降。用通俗的话来讲就是模型的应试能力很强，实际应用表现就很差。这种情况就是过拟合。</p>
<p data-nodeid="3187">那 L2 是如何解决过拟合的呢？</p>
<p data-nodeid="3188">首先，参数值小的模型一般会比较简单，它能适应不同的数据集，也能在一定程度上避免过拟合。举一个简单的例子，y=10*x<sub>1</sub>+1000*x<sub>2</sub>。x<sub>2</sub> 取值 1 和 3，最终会导致 y 差了 2000。x<sub>2</sub> 稍微一变，结果就差别很大了。更进一步，x<sub>1</sub> 的权重就低了很多，x<sub>1</sub> 的变化对 y 的结果影响小，模型的适应性就很差了。所以，我们可以得出一个结论：<strong data-nodeid="3392">越小的参数，模型就越简单，越简单的模型，就越不容易过拟合</strong>。</p>
<p data-nodeid="3189">我们再回头看公式，可以看到，L2 实际上就是让向量中所有元素的平方和再开方。那么，如果我们要避免模型过拟合，就要使 L2 最小，这意味着向量中的每一个元素的平方都要尽量小，且接近于 0。和 L0 和 L1 不一样，L2 不会让元素等于 0。</p>
<p data-nodeid="3190">由此，L1 和 L2 范数的区别就很显然了。</p>
<p data-nodeid="3191">L1 会趋向于产生少量的特征，而其他的特征都是 0，用于特征选择和稀疏；L2 会选择更多的特征，但这些特征都会接近于 0，用于减少过拟合。</p>
<h3 data-nodeid="3192">微积分部分概念回顾</h3>
<p data-nodeid="3193">微积分是现代数学的核心基础知识。在本课时，我会带你回顾一些本课程中会用到的知识点，分别是导数、偏导数和梯度。我们先从导数开始。</p>
<h4 data-nodeid="3194">1. 导数</h4>
<p data-nodeid="3195">导数，也叫作导函数值。</p>
<p data-nodeid="3196">假定我们现在手头有一个函数 F(x) = 2x。当 x=1 的时候，函数值 F(x)=F(2)=2*1。然后我们给 x 增加一个非常小的变化（增量）Δx，那么F(x+Δx)=2(x+Δx)，这意味着函数的结果也有了一个增量，记为Δy。</p>
<p data-nodeid="3197">当函数值增量Δy 与变量增量Δx 的比值在Δx 趋近于 0 时，如果极限 a 存在，我们就称 a 为函数 F(x)在 x 处的导数。</p>
<p data-nodeid="3198">这里有两个需要注意的地方，<strong data-nodeid="3415">第一个是Δx 一定要趋近于 0</strong>，<strong data-nodeid="3416">第二个是极限 a 要存在</strong>。</p>
<p data-nodeid="3199">F(x) = 2x 的图像如下：</p>
<p data-nodeid="3200"><img src="https://s0.lgstatic.com/i/image/M00/61/93/CgqCHl-P8IOAD6XqAAC899Bz0Og903.png" alt="Drawing 9.png" data-nodeid="3420"></p>
<p data-nodeid="3201">不难发现，在这个函数中，导数实际上就是咱们平时说的斜率。一个函数在某一点的导数描述了这个函数在这一点附近的变化率。</p>
<p data-nodeid="3202">导数一般我们记为：</p>
<p data-nodeid="3203"><img src="https://s0.lgstatic.com/i/image/M00/61/93/CgqCHl-P8IqAf-pxAAEDipHzTUk023.png" alt="Drawing 10.png" data-nodeid="3425"></p>
<p data-nodeid="3204">其中 lim 为“极限”的意思。也可记为：</p>
<p data-nodeid="3205"><img src="https://s0.lgstatic.com/i/image/M00/61/FE/CgqCHl-RITyAABDmAAAiPie5pLo813.png" alt="Lark20201022-140519.png" data-nodeid="3429"></p>
<blockquote data-nodeid="3206">
<p data-nodeid="3207">不光函数有导数，导数也有导数。代表函数在 x 处斜率（导数）的变化率我们称之为二阶导数。由此类推，还有高阶导数等。</p>
</blockquote>
<h4 data-nodeid="3208">2. 偏导数</h4>
<p data-nodeid="3209">我们再来看第二个概念，偏导数。</p>
<p data-nodeid="3210">在实际应用中，很多函数都有多个变量。为了方便分析不同变量与函数的关系，为单个变量求导是很有必要的。这个时候，我们需要让其他变量不变，只有某一个变量发生变化，这种情况下的求导我们称之为“<strong data-nodeid="3440">偏导数</strong>”。公式如下：</p>
<p data-nodeid="3211"><img src="https://s0.lgstatic.com/i/image/M00/61/87/Ciqc1F-P8J2AOqIzAAJG3-Hg-rg423.png" alt="Drawing 12.png" data-nodeid="3443"></p>
<p data-nodeid="3212">刚才说过，导数就是函数在某个点上的斜率。如果我们把坐标系从二维变成三维，甚至更多维时，偏导数就不难理解了：<strong data-nodeid="3449">它实际上是函数在不同方向（坐标轴）上的变化率</strong>。就好比爬山，任意一个位置，都会有东西方向和南北方向的坡度（斜率）。</p>
<p data-nodeid="3213">假定我们有一个函数 z = f(x,y)，我们想要求这个函数在 x 方向的导数，只需要将 y 固定，在 x 上增加一个小的增量Δx；同样的，如果要求 y 方向的导数，则需要固定 x，y 上增加一个增量Δy。</p>
<p data-nodeid="3214">例如：</p>
<p data-nodeid="3215"><img src="https://s0.lgstatic.com/i/image/M00/61/FE/CgqCHl-RIUyAPd4PAAAs2r9vDHs767.png" alt="Lark20201022-140523.png" data-nodeid="3454"></p>
<p data-nodeid="3216"><img src="https://s0.lgstatic.com/i/image/M00/61/FE/CgqCHl-RIWmAIRNjAAAvEHSnO74164.png" alt="Lark20201022-140621.png" data-nodeid="3457"></p>
<p data-nodeid="3217">表示函数在 x 轴方向上的导数</p>
<p data-nodeid="3218"><img src="https://s0.lgstatic.com/i/image/M00/61/FE/CgqCHl-RIXOATmX_AAA1jeeItHQ994.png" alt="Lark20201022-140618.png" data-nodeid="3461"></p>
<p data-nodeid="3219">表示函数在 y 轴方向上的导数。</p>
<h4 data-nodeid="3220">3. 梯度</h4>
<p data-nodeid="3221">在机器学习中，梯度是一个出现频率极高的词语，模型的设计、训练、优化等过程中，梯度都是一个核心概念。函数的所有偏导数构成的向量就叫作梯度。我们用∇f 表示，公式化的形式为：</p>
<p data-nodeid="3222"><img src="https://s0.lgstatic.com/i/image/M00/61/93/CgqCHl-P8MWAXiznAACvaXF4Y_M570.png" alt="Drawing 16.png" data-nodeid="3469"></p>
<p data-nodeid="3223"><strong data-nodeid="3473">一定要注意，梯度是一个向量。同时，梯度向量的方向即为函数值增长最快的方向。</strong></p>
<p data-nodeid="3224">在后续的课时中，我将会介绍损失函数、反向传播、优化方法等内容，它们都离不开梯度。</p>
<p data-nodeid="3225">接下来，我们一同进入本节课的最后一个部分“<strong data-nodeid="3480">信息论</strong>”。</p>
<h3 data-nodeid="3226">信息论部分知识点回顾</h3>
<p data-nodeid="3227">信息论，在深度学习中是一个非常重要的概念，它集成了微积分、概率论和统计学中很多的概念和想法。它的应用场景非常多，像损失函数中的交叉熵损失、机器学习中构建决策树使用到的信息增益、NLP 和语音算法中的维特比算法等。</p>
<p data-nodeid="3228">提到信息论，就不得不从最基础的熵的定义开始。</p>
<h4 data-nodeid="3229">1. 熵</h4>
<p data-nodeid="3230">熵，也称信息熵。</p>
<p data-nodeid="3231">假定我们有一枚标准的硬币，每次投掷，得到正面和反面的概率都是 1/2，不确定性非常大。假如我们在硬币上做点手脚，在正面加点重量，那么每次投掷的概率就发生了变化，正面朝上的概率就比原来大了，比如此时变成了 2/3，这种正反面的不确定性就减少了。</p>
<p data-nodeid="3232">对于每一个事件（情况）的发生，都有一个信息量的度量，它的公式为：</p>
<p data-nodeid="3233"><img src="https://s0.lgstatic.com/i/image/M00/61/88/Ciqc1F-P8NKASOb6AABQIYI4l00002.png" alt="Drawing 17.png" data-nodeid="3492"></p>
<p data-nodeid="3234">其中 P(x) 是 x 发生的概率。</p>
<p data-nodeid="3235">以投掷硬币来说，投硬币是有正反两种不确定性（概率为 p<sub>i</sub>）的。我们将这两种不确定性的总量进行量化，就成了: -log(P(正面)) - log(P(反面))。</p>
<p data-nodeid="3236">当我们把投掷硬币推广到更广义的场景下，就得到了熵的定义公式，如下：</p>
<p data-nodeid="3237"><img src="https://s0.lgstatic.com/i/image/M00/61/88/Ciqc1F-P8NqAGD5YAABrQs1-1Ec962.png" alt="Drawing 18.png" data-nodeid="3502"></p>
<p data-nodeid="3238">这个公示就是信息熵的公式，其中 p(x<sub>i</sub>)就是各个可能事件发生的概率。通过公示可以看出，熵越大，不确定性越大。</p>
<h4 data-nodeid="3239">2. KL 散度</h4>
<p data-nodeid="3240"><strong data-nodeid="3515">KL 散度，也称为相对熵，它衡量了两个分布之间的差异</strong>。我们来看它的公式：</p>
<p data-nodeid="3241"><img src="https://s0.lgstatic.com/i/image/M00/61/93/CgqCHl-P8OiAIEfmAADIqH1g_v4032.png" alt="Drawing 19.png" data-nodeid="3518"></p>
<p data-nodeid="3242">上面的 p(x<sub>i</sub>) 为真实事件的概率分布，q(x<sub>i</sub>) 为理论拟合出来的该事件的概率分布。因此该公式字面上的含义就是真实事件的信息熵，同理论拟合的事件的信息量与真实事件的概率的乘积的差的累加。</p>
<p data-nodeid="3243">这句话非常的长，我们拆开来看，真实事件的信息熵就是 p(x<sub>i</sub>) log p(x<sub>i</sub>)，理论拟合的事件的信息量就是 log q(x<sub>i</sub>)，真实事件的概率就是 p(x<sub>i</sub>)。</p>
<p data-nodeid="3244">在模型优化、数据分析和统计等场合下，我们就可以使用 KL 散度衡量我们选择的近似分布与数据原分布有多大差异。当拟合事件和真实事件一致的时候 KL 散度就成了 0，不一样的时候就大于 0。</p>
<h4 data-nodeid="3245">3. 交叉熵</h4>
<p data-nodeid="3246">交叉熵也衡量了两个分布之间的差异，但是与 KL 散度的区别在于，交叉熵代表用拟合分布来表示实际分布的困难程度，其公式如下：</p>
<p data-nodeid="3247"><img src="https://s0.lgstatic.com/i/image/M00/62/24/Ciqc1F-RYr2ATA-YAADcPevzyG8249.png" alt="Lark20201022-184437.png" data-nodeid="3552"></p>
<p data-nodeid="3248">三种度量方式的公式，如果我们放在一起看，就发现其中的关联，如下所示：</p>
<p data-nodeid="3249"><img src="https://s0.lgstatic.com/i/image/M00/61/88/Ciqc1F-P8QOAIOjhAABVYFHz5BU651.png" alt="Drawing 21.png" data-nodeid="3556"></p>
<h3 data-nodeid="3250">结语</h3>
<p data-nodeid="3251">深度学习是一个很庞大的学科，用到的知识点也非常多，但是这不意味着你需要重新翻箱倒柜把大学的书本拿出来继续啃。本课时我对未来会用到的线性代数、微积分、信息论中的核心内容进行了介绍，在后续的课程中，我还会对遇到的知识进行及时的补充。</p>
<p data-nodeid="3252">你对深度学习中的哪些数学概念感兴趣，可以留言告诉我，咱们一起讨论。</p>
<p data-nodeid="3253">下一课时，我将带你了解，什么是神经元。</p>

---

### 精选评论

##### *浩：
> 老师，讲的很细致，但是之前接触的不多，可以有一些案例实战吗？

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 会涉及到案例实战，在模块3中。

前两个模块主要以基础概念和工具学习为主，对于重要的环节，比如CNN、自动编码机等，都会有具体的使用代码进行介绍。

##### **烁：
> 老师为什么参数越小，模型越简单？谢谢老师

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 可以反过来想，越复杂的模型一般就越能更好地拟合问题（样本点），拟合的点也越多。点多了，意味着异常的样本点也可能会被拟合。异常点会导致预测值的异常波动。这随之带来的问题就比如导数异常波动等。所以参数越小，模型越简单。

##### **烁：
> 试想一下，如果我们有一个线性回归方程，其中某个参数非常大，那这个参数稍微变化一下，最终结果的区别就会很大，模型的适应性也会下降。老师这个能这个具体方程的例子吗，看不懂。谢谢老师

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 我们不妨通过两个阶段来看。

1. 学习阶段，这个阶段参数w2=1000这个数字是没有学到的，模型还在试探（更新）的阶段，那么如果w2总是一些很大的值，那么在学习的阶段，损失函数的值就会有很大的波动。

2. 学习后也就是使用的时候，这个阶段模型的参数是固定住的，比如w2=1000，那么如果x2变化，就会导致最终的y变化（波动）很大，也会对模型的结果产生影响。


需要注意的是，文章中的解释，实际上是针对学习阶段的，它的影响将会集中在对损失函数、梯度方面的影响。

##### **勇：
> 终于明白L0,L1,L2的作用了😂

##### sky：
> 想了解怎么样学习机器学习的数学知识

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 机器学习研发职位，在实际中分为研究和应用两种，研究性质的职位，确实要对数学有很深的造诣，不妨采用根据你使用的算法针对性的去复习或者学习对应数学知识的方式会好一些。而对于应用性质的，主要侧重的是问题的分析、归纳、与算法结合等几个方面，对于数学的要求，则没有那么高。

##### **升：
> 老师对于“L1 会趋向于产生少量的特征，而其他的特征都是 0，用于特征选择和稀疏；L2 会选择更多的特征，但这些特征都会接近于 0，用于减少过拟合”我不太理解，能讲解下为什么是这样嘛？？？

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 回到L1，L2的介绍看定义。此外，接近于零意味着模型的波动性小，不会过于拟合异常点的情况，也就不会容易过拟合了。

##### **闯：
> 老师很棒，我会更棒的😀

##### *飞：
> 讲的很棒，收获很多，期待！

##### **9831：
> 讲的真好😀😀😀

##### **2001：
> 深度学习背后的核心是标量、向量、矩阵和张量这 4 种数据结构。向量的点乘，也叫向量的内积、数量积，要求两个向量的长度一致，点乘的结果是一个标量。向量的叉乘，也叫向量的外积、向量积。叉乘的运算结果是一个向量而不是一个标量。叉乘用得较少。对应项相乘，顾名思义，就是两个向量对应的位置相乘，得到的结果还是原来的形状。L0 范数和 L1 范数都能实现权值稀疏。但 L1 范数是 L0 范数的最优凸近似。L2 也有它的作用，那就是防止过拟合。L1 会趋向于产生少量的特征，而其他的特征都是 0，用于特征选择和稀疏；L2 会选择更多的特征，但这些特征都会接近于 0，用于减少过拟合。

##### **0356：
> 没学过信息论影响大吗😂

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 没有影响

##### *震：
> 感觉回到了大学 线性代数课堂

##### *岩：
> 看了老师的课，疑惑一点点的解开了

##### **言：
> 老师讲的很实在，很实用，希望能拿到更多offer

