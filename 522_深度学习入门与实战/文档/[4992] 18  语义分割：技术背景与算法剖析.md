<p data-nodeid="1773" class="">上两讲，我向你介绍了图片分类的原理与实践。这一讲，我将向你介绍“语义分割”，它与图片分类都是计算机视觉中的重要任务。</p>
<h3 data-nodeid="1774">图像语义分割任务介绍</h3>
<p data-nodeid="1775">在图像分类任务中，我们要做的是从图片中提取丰富的特征信息。通常我们会通过 Softmax 或者 Sigmoid 函数，将提取到的特征信息转换为对应的类别。</p>
<p data-nodeid="1776">例如一张卧室的图片，我们通过卷积层从图片中提取特征信息后，如果是多分类，可以使用 Softmax 函数获得这张图最有可能所属的几种类别（卧室、床、台灯等）；如果是二分类，可以通过 Sigmoid 函数获得这张图所属的类别（卧室还是床）。</p>
<p data-nodeid="1777"><img src="https://s0.lgstatic.com/i/image2/M01/01/21/Cip5yF_YI-uAQuXuADLaCOWWbXk832.png" alt="Drawing 0.png" data-nodeid="1934"></p>
<div data-nodeid="1778"><p style="text-align:center">图 1：卧室</p></div>
<p data-nodeid="1779">在语义分割中，我们同样要提取图片中的特征信息，只不过不是对图片的整体分类，而是对图像中的每一个像素分类，确定每一个像素所属的类别。</p>
<p data-nodeid="1780">当我们对上图进行语义分割，不同的颜色代表不同的类别，属于同一个物体的会被分为同一个类别：台灯所在的区域被分为蓝色，床所在的区域被分为墨绿色。</p>
<p data-nodeid="1781"><img src="https://s0.lgstatic.com/i/image/M00/89/44/Ciqc1F_YI_iAb2pRAAC9d-5i6iY807.png" alt="Drawing 1.png" data-nodeid="1939"></p>
<div data-nodeid="1782"><p style="text-align:center">图 2：语义分割后的卧室</p></div>
<blockquote data-nodeid="1783">
<p data-nodeid="1784">图 1、图 2 来源 <a href="http://people.csail.mit.edu/bzhou/publication/scene-parse-camera-ready.pdf" data-nodeid="1943">Scene Parsing through ADE20K Dataset</a> 和 <a href="https://arxiv.org/pdf/1608.05442.pdf" data-nodeid="1947">Semantic Understanding of Scenes through ADE20K Dataset</a>。</p>
</blockquote>
<p data-nodeid="1785">深度学习崛起之前，在计算机视觉领域进行图像分割还是一个难题。但随着深度学习的发展，图像分割中涌现出相当多的优秀算法和模型，使得图像分割在越来越多的领域中得到应用，例如自动驾驶、人机交互、医学影像处理，如下所示：</p>
<p data-nodeid="1786"><img src="https://s0.lgstatic.com/i/image2/M01/01/21/Cip5yF_YJBqAEcpEABjzcmAbpLM206.png" alt="Drawing 2.png" data-nodeid="1952"></p>
<div data-nodeid="1787"><p style="text-align:center">图 3：自动驾驶</p></div>
<blockquote data-nodeid="1788">
<p data-nodeid="1789">图 3 来源 <a href="https://www.cityscapes-dataset.com/wordpress/wp-content/papercite-data/pdf/cordts2016cityscapes.pdf" data-nodeid="1956">The Cityscapes Dataset for Semantic Urban Scene Understanding</a>。</p>
</blockquote>
<p data-nodeid="1790"><strong data-nodeid="1961">语义分割算法与图像分类算法</strong></p>
<p data-nodeid="1791">在图像分类问题中，输入的图片会通过卷积神经网络转换成相应的特征信息。通常这些特征信息会通过一个或者几个全连接层转换为一个标量，这个标量就是我们分类的标签。就像下面这个手写字体识别的例子：</p>
<p data-nodeid="1792"><img src="https://s0.lgstatic.com/i/image2/M01/01/22/CgpVE1_YJCKAREihAAMPv28rg_g652.png" alt="Drawing 3.png" data-nodeid="1965"></p>
<div data-nodeid="1793"><p style="text-align:center">图 4：手写字体识别</p></div>
<p data-nodeid="1794">在最后的全连接层，会将前面的信息转换为一个维度为 10 的标量，最后一层中每一个神经元的输出就是输入图片所属的类别。</p>
<p data-nodeid="1795">语义分割的任务中，需要输出 1 张二维的特征，分割图中的每个像素代表的是输入图片对应像素的类别。那如何将最后的输出变成 1 张二维的特征图呢？<strong data-nodeid="1972">丢弃全连接层，换上全卷积层</strong>。</p>
<p data-nodeid="1796">通过下面这张图我们再解释一下两者的区别。</p>
<p data-nodeid="1797"><img src="https://s0.lgstatic.com/i/image2/M01/01/21/Cip5yF_YJCqAUzwGAAQfFLUIAI8588.png" alt="Drawing 4.png" data-nodeid="1976"></p>
<div data-nodeid="1798"><p style="text-align:center">图 5：含全连接层的 CNN 与全卷积的 CNN</p></div>
<p data-nodeid="1799">假设我们要将 1 张图片分为 1000 类和分割成 1000 个类别。</p>
<p data-nodeid="1800">图中的上半部分要将提取信息后，通过有 1000 个节点的全连接层，计算出每个类别的概率。tabby cat 这个类别的概率最高，我们就可以判断这张图为 tabby cat。</p>
<p data-nodeid="1801">那分割时呢？我们需要推断出每个像素所属的类别，最后的全连接层换成有 1000 个卷积核的卷积层，输出的特征图就会有 1000 个通道，在这 1000 个通道每个像素点的位置上计算 argmax 就可以获得对应类别。</p>
<p data-nodeid="1802">那如何构建这样的分割网络呢？最简单的办法就是堆叠卷积层，通过控制 padding 的大小保证每一层输出的特征图的大小都是相同的，如图所示：</p>
<p data-nodeid="1803"><img src="https://s0.lgstatic.com/i/image/M00/89/44/Ciqc1F_YJDWARGToAAdH4AXS684142.png" alt="Drawing 5.png" data-nodeid="1983"></p>
<div data-nodeid="1804"><p style="text-align:center">图 6：简单语义分割网络示意图</p></div>
<p data-nodeid="1805">1 张 3 通道的图片，高、宽分别为 H、W。将图片作为输入，经过一系列的卷积操作，获得（C，H，W）的特征图，然后再计算每一个像素对应的类别。</p>
<p data-nodeid="1806">通常最终会输出宽高与原图相同的二维数组，二维数组的每个值记录了原图中对应像素的类别。这个二维数组可视化后就是上图最右侧的 Predictions。</p>
<p data-nodeid="1807">这种方式很简单，但会造成了 2 个问题：</p>
<ul data-nodeid="1808">
<li data-nodeid="1809">
<p data-nodeid="1810"><strong data-nodeid="1991">感受野维持不变</strong>。我讲过池化（pooling）可以扩大感受野，感受野越大可以使分割网络越精细。但在上面的网络中，不会用到池化，所以它的感受野也不会发生变化。</p>
</li>
<li data-nodeid="1811">
<p data-nodeid="1812"><strong data-nodeid="1996">计算量大，网络难以优化</strong>。</p>
</li>
</ul>
<p data-nodeid="1813">在这里，我们面临了与图像分类的同样问题，网络该如何设计？解决方案与图像分类相同：采用业界高手为我们设计好的经典分割算法。因此，下面我会向你介绍 2 种经典的语义分割模型， FCN 与 U-Net。</p>
<h3 data-nodeid="1814">经典语义分割模型</h3>
<p data-nodeid="1815">在介绍之前，我们再回顾一下训练一个模型必不可少的 4 个元素。</p>
<ul data-nodeid="1816">
<li data-nodeid="1817">
<p data-nodeid="1818">数据：主要是训练集与评估集，用来训练与评估我们的模型。</p>
</li>
<li data-nodeid="1819">
<p data-nodeid="1820">网络结构：也就是我们模型的主体。</p>
</li>
<li data-nodeid="1821">
<p data-nodeid="1822">损失函数：更新模型参数的核心。</p>
</li>
<li data-nodeid="1823">
<p data-nodeid="1824">优化方法：更新模型参数的方法。</p>
</li>
</ul>
<p data-nodeid="1825">语义分割模型的数据如何标注，我会在下一讲介绍。</p>
<p data-nodeid="1826">我们之前讲过的损失函数与优化方法也适用于语义分割模型训练。语义分割模型与图像分割模型在训练角度上看，唯一区别就是网络结构。</p>
<h4 data-nodeid="1827">FCN</h4>
<p data-nodeid="1828">FCN 算是卷积神经网络在语义分割中的开山之作。你可以点击<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" data-nodeid="2010">链接</a>查看 FCN 的论文。</p>
<p data-nodeid="1829">FCN 主要有 3 个创新点：</p>
<ol data-nodeid="1830">
<li data-nodeid="1831">
<p data-nodeid="1832">采用全卷积神经网络，解决了像素级别的预测问题；</p>
</li>
<li data-nodeid="1833">
<p data-nodeid="1834">使用上采样操作恢复图像原有尺寸，实现像素级预测；</p>
</li>
<li data-nodeid="1835">
<p data-nodeid="1836">通过跨层连接融合了网络的浅层与深层次的信息。</p>
</li>
</ol>
<p data-nodeid="1837">为什么采用全卷积神经网络刚才已经介绍了，我们直接来看一下创新点 2 与创新点 3。</p>
<p data-nodeid="1838"><strong data-nodeid="2020">上采样</strong></p>
<p data-nodeid="1839">创新点 2 是“使用上采样操作恢复图像原有尺寸，实现像素级预测”，那什么是上采样呢？</p>
<p data-nodeid="1840">利用卷积与池化提取特征的过程中会让特征图逐渐变小。为了能进行像素级别的预测，我们需要将提取到的信息放大到原始的尺寸。放大特征图的方式就是上采样。上采样通常有 3 种方式：</p>
<ul data-nodeid="1841">
<li data-nodeid="1842">
<p data-nodeid="1843">Resize，双线性插值；</p>
</li>
<li data-nodeid="1844">
<p data-nodeid="1845">转置卷积；</p>
</li>
<li data-nodeid="1846">
<p data-nodeid="1847">反池化。</p>
</li>
</ul>
<p data-nodeid="1848">双线性插值属于图像相关的操作，反池化非常简单，用得也比较少。因此，这里我主要介绍转置卷积。</p>
<p data-nodeid="1849">转置卷积是一种比较特殊的卷积操作，它会先按照一定的方式对输入的特征图补 0 来扩大尺寸，然后再进行卷积操作。</p>
<p data-nodeid="1850">我们先复习一下原始的卷积操作。</p>
<p data-nodeid="1851">下图蓝色部分为一个 4x4 的特征图，在这个特征图上应用 3x3 的卷积（深蓝色部分），padding=0，stride=1，输出为一个 2x2 的特征图。</p>
<p data-nodeid="1852"><img src="https://s0.lgstatic.com/i/image2/M01/01/22/CgpVE1_YJGSAJeWDAAAvgGRRJik963.png" alt="Drawing 6.png" data-nodeid="2032"></p>
<div data-nodeid="1853"><p style="text-align:center">图 7</p></div>
<blockquote data-nodeid="1854">
<p data-nodeid="1855">图片来源于 <a href="https://github.com/vdumoulin/conv_arithmetic" data-nodeid="2036">GitHub</a>。</p>
</blockquote>
<p data-nodeid="1856">我们学过卷积的计算方式，可以知道输入值和输出值之间存在位置上的连接关系。输入特征图左上方的数据会影响到输出矩阵的左上方的值，一个卷积操作是一个多对一的映射关系。</p>
<p data-nodeid="1857">上图中，输入特征图中的 9 个元素应用卷积操作后会生成输出特征图中的一个数值。而转置卷积则是与正常卷积相反的一个操作，是一对多的关系：它需要<strong data-nodeid="2044">将输入特征图中的一个元素经过卷积操作后，生成输出特征图中的多个数值</strong>。</p>
<p data-nodeid="1858">我们将上图中的卷积操作进行矩阵化处理，输入的特征图为 X，卷积为 K，输出为 Y。</p>
<p data-nodeid="4778" class=""><img src="https://s0.lgstatic.com/i/image/M00/8B/D0/CgqCHl_gWLaAWC9fAACZydIpZ3E145.png" alt="图片3.png" data-nodeid="4781"><br>
<img src="https://s0.lgstatic.com/i/image/M00/8B/D0/CgqCHl_gWMCAXj1zAABp0HZ_hUA228.png" alt="图片4.png" data-nodeid="4785"><br>
<img src="https://s0.lgstatic.com/i/image/M00/8B/C5/Ciqc1F_gWMiAZlheAAA1onFg--w777.png" alt="图片5.png" data-nodeid="4789"></p>







<p data-nodeid="1862">Y=K*X，*代表卷积操作。我们将 K 转换为一个矩阵 C，矩阵的每一行代表一个卷积操作，定义如下：</p>
<p data-nodeid="5652" class=""><img src="https://s0.lgstatic.com/i/image/M00/8B/D0/CgqCHl_gWNqACxiEAAEXvcPl758759.png" alt="图片2.png" data-nodeid="5655"></p>

<p data-nodeid="1864">为了将卷积操作转换为矩阵计算，我们还需要将输入 X 转换为向量的模式，我们定义为 X'：</p>
<p data-nodeid="6518" class=""><img src="https://s0.lgstatic.com/i/image/M00/8B/D0/CgqCHl_gWOKANulTAABfKyOZVaY065.png" alt="图片1.png" data-nodeid="6521"></p>

<p data-nodeid="1866">这样的话，输入的特征图为 X'，卷积矩阵为 C，输出的特征图为 Y。卷积变换可以转换为如下的矩阵计算 Y=CX'：</p>
<p data-nodeid="7384" class=""><img src="https://s0.lgstatic.com/i/image/M00/8B/C5/Ciqc1F_gWO2AOWTmAAEZezef3_U767.png" alt="图片6.png" data-nodeid="7387"></p>

<p data-nodeid="1868">根据矩阵的计算方式可以知道，矩阵 C 的维度是 4x16，我们可以把一个 4x4 的特征转换为 2x2 的特征。那如果我们有一个 16x4 的卷积矩阵，是不是就可以把 2x2 的特征，恢复到 4x4 了？</p>
<p data-nodeid="1869">我们将卷积矩阵求转置，获得维度为 16x4 的转置矩阵 C<sup>T</sup>。</p>
<p data-nodeid="1870">用 16x4 的 C<sup>T</sup> 对 4x1 的 Y 的列向量进行矩阵计算，就可以获得一个 16x1 的向量，将这个列向量展开就可以获得一个 4x4 的特征图。这就是转置卷积的原理。</p>
<p data-nodeid="1871">不过希望你注意一下，<strong data-nodeid="2097">转置卷积的参数是通过学习获得到的，只能恢复到原有特征图的形状，不能获得原有特征图的内容</strong>。你可以到<a href="https://github.com/vdumoulin/conv_arithmetic" data-nodeid="2095">网站</a>上看到更多有关卷积与转置卷积的动态说明图片。</p>
<p data-nodeid="1872"><strong data-nodeid="2101">跨层连接</strong></p>
<p data-nodeid="1873">创新点 3 是跨层连接，它“通过跨层连接融合了网络的浅层与深层次的信息”。</p>
<p data-nodeid="1874">我们先来看 FCN 的结构：</p>
<p data-nodeid="1875"><img src="https://s0.lgstatic.com/i/image2/M01/01/22/Cip5yF_YJJyAGsirAAJ96jlATxI100.png" alt="Drawing 13.png" data-nodeid="2106"></p>
<div data-nodeid="1876"><p style="text-align:center">图 8：FCN 结构</p></div>
<p data-nodeid="1877">第一行直接对 pool5 的输出做 32 倍的上采样，然后对上采样中的每个像素点做 Softmax 预测，就得到了 32x upsampled prediction（FCN-32s）。这样一步到位的恢复到原始输入，会损失过多信息，效果非常不准。</p>
<p data-nodeid="1878">为了解决上面的问题，FCN 的作者先将 pool5 进行 2 倍的上采样，2 倍上采样后的大小与 pool4 相同。然后他将上采样后的结果与 pool4 相加，再进行 16 倍的上采样，再对每个像素做 Softmax 预测，就得到了 FCN-16s，效果有所提升，但仍然一般。</p>
<p data-nodeid="1879">因此，作者再次将上采样的倍数减少，对 pool5 与 pool4 先做上采样，使得它们的尺寸与 pool3 相同，然后将特征图相加。这时只需要做 8 倍的上采样就可以了。</p>
<p data-nodeid="1880">下面是 FCN-32s、FCN-16s、FCN-8s 的对比图：</p>
<p data-nodeid="1881"><img src="https://s0.lgstatic.com/i/image2/M01/01/23/CgpVE1_YJKOATjmWAAEhGaK8OGY552.png" alt="Drawing 14.png" data-nodeid="2113"></p>
<div data-nodeid="1882"><p style="text-align:center">图 9：FCN-32s、FCN-16s、FCN-8s 的对比图</p></div>
<h4 data-nodeid="1883">U-Net</h4>
<p data-nodeid="1884">我再来介绍一个经典的语义分割网络，U-Net。</p>
<p data-nodeid="1885">U-Net 可以说是非常精简且高效的语义分割模型，无论在服务器端还是移动终端，都取得了非常好的效果。你可以点击<a href="https://arxiv.org/pdf/1505.04597.pdf" data-nodeid="2119">链接</a>查看 U-Net 的论文。</p>
<p data-nodeid="1886">U-Net 是基于 Encoder-Decoder 架构开发的，因此，我们通过 U-Net 分割网络来了解 Encoder-Decoder 架构。</p>
<p data-nodeid="1887">首先我们来看看 U-Net 的网络结构：</p>
<p data-nodeid="1888"><img src="https://s0.lgstatic.com/i/image2/M01/01/22/Cip5yF_YJK-AQeYGAADKHS2STjU887.png" alt="Drawing 15.png" data-nodeid="2125"></p>
<div data-nodeid="1889"><p style="text-align:center">图 10：U-Net 的网络结构</p></div>
<p data-nodeid="1890">U-Net 的网络结构呈现的是一个“U”型，U 的左侧是一个收缩的过程，就是我们刚才所说的 Encoder 过程；U 的右边是一个扩张过程，即 Decoder 过程。</p>
<p data-nodeid="1891">Decoder 过程是为了让我们的特征图渐渐转换成一副一定尺寸的二维特征图，这个二维特征图中包含着每一个像素的类别，它的尺寸（宽与高）取决于具体业务要求，大多数情况下会与输入的图片保持一致。</p>
<p data-nodeid="1892">在 U-Net 中主要有 3x3 的 conv+ReLU、2x2 的 pooling、2x2 的 Up-conv、copy and crop</p>
<p data-nodeid="1893">以及 1x1 的 conv 这 4 种操作。下面让我们来看看它们具体有什么作用。</p>
<ul data-nodeid="1894">
<li data-nodeid="1895">
<p data-nodeid="1896"><strong data-nodeid="2134">3x3 conv + ReLU</strong>（图中蓝色的剪头）：在这个网络中，蓝色的长条代表网络中的特征图，每 3 个蓝色的长条可以看作一个单元，在每个单元中，由 2 个 3x3 的卷积层和 RelU 完成特征提取。</p>
</li>
<li data-nodeid="1897">
<p data-nodeid="1898"><strong data-nodeid="2139">2x2 的 pooling</strong>（图中红色向下的剪头）：在 Encoder 阶段中，每个单元通过一个 2x2、stride=2 的 pooling 操作来完成一个下采样操作。</p>
</li>
<li data-nodeid="1899">
<p data-nodeid="1900"><strong data-nodeid="2144">2x2 的 up-conv</strong>（绿色向上的剪头）：在 Decoder 阶段，通过一个 2x2 的上采样操作完成一个上采样操作。</p>
</li>
<li data-nodeid="1901">
<p data-nodeid="1902"><strong data-nodeid="2149">copy and crop</strong>：参考 ResNet，U-Net 中也采用了跳层连接，以保证前面学习到的内容也可以很好地传播到后面的层中。</p>
</li>
<li data-nodeid="1903">
<p data-nodeid="1904"><strong data-nodeid="2154">1x1 的 conv</strong>：在最后一层，将 64 个特征图映射为一个我们需要的分割结果。在这一操作过程中，我们分割类别的数量会决定最后输出特征图的通道数。</p>
</li>
</ul>
<p data-nodeid="1905">这就是 UNet 的网络架构，是不是很简单？在后面的学习中，我会带你用 U-Net 来做一个实际的分割项目。</p>
<h3 data-nodeid="1906">常用数据集</h3>
<p data-nodeid="1907">我们如果想训练一个语义分割的模型，必须要有数据。语义分割需要的数据相对图像分割来说比较难获得到，需要人工使用 Labelme 等软件进行标注，在下一讲我会告诉你如何标注数据。</p>
<p data-nodeid="1908">不过业界也有几个比较知名的公开数据集，你可以使用这些数据来进行试验，通常发论文也是综合对比模型在这些数据集上的效果如何的。</p>
<h4 data-nodeid="1909">COCO 数据集</h4>
<p data-nodeid="1910">COCO 是一个大规模物体检测、图像分割的数据集。图像分割部分的数据一共有 80 个类别。下载地址为<a href="https://cocodataset.org/#download" data-nodeid="2163">https://cocodataset.org/#download</a>。</p>
<h4 data-nodeid="1911">PASCAL-Context Dataset</h4>
<p data-nodeid="1912">PASCAL-Context Dataset 在 PASCAL 数据的基础上补充了数据，训练和验证集包含 10,103 张图像，测试集包含 9,637 张图像。下载地址为<a href="https://cs.stanford.edu/~roozbeh/pascal-context/" data-nodeid="2171">https://cs.stanford.edu/~roozbeh/pascal-context/</a>。</p>
<p data-nodeid="1913"><img src="https://s0.lgstatic.com/i/image2/M01/01/22/Cip5yF_YJLqAJXv9AA3Wk64H6xw932.png" alt="Drawing 16.png" data-nodeid="2175"></p>
<div data-nodeid="1914"><p style="text-align:center">图 11：PASCAL-Context Dataset</p></div>
<h4 data-nodeid="1915">MIT Scene Parsing Benchark</h4>
<p data-nodeid="1916">用场景解析的数据集。共有 150 个类别，共有 20000 张图片。下载地址为<a href="http://sceneparsing.csail.mit.edu/" data-nodeid="2180">http://sceneparsing.csail.mit.edu/</a>。</p>
<h4 data-nodeid="1917">CitySpace</h4>
<p data-nodeid="1918">有来自 50 个城市的街景数据，一共有 50 个类别。5000 张图片被精准标记过，20000 张图片被粗糙的标记过。下载地址为<a href="https://www.cityscapes-dataset.com/" data-nodeid="2186">https://www.cityscapes-dataset.com/</a>。</p>
<p data-nodeid="1919"><img src="https://s0.lgstatic.com/i/image2/M01/01/23/CgpVE1_YJMWAU8i9AA1vXpaBcP8803.png" alt="Drawing 17.png" data-nodeid="2190"></p>
<div data-nodeid="1920"><p style="text-align:center">图 12：CitySpace</p></div>
<h3 data-nodeid="1921">结语</h3>
<p data-nodeid="1922">这一讲我介绍了计算机视觉领域中一个非常重要的应用，语义分割。同时还介绍了语义分割中两个相对简单的算法，FCN 与 U-Net。FCN 算是卷积神经网络在语义分割领域中的开山之作，U-Net 虽然简单，但非常高效。</p>
<p data-nodeid="1923">那么我想提一个小问题：<strong data-nodeid="2199">假如一张图片的尺寸是 1024*720，语义分割网络输出的特征图是 80x80 可不可以呢？如果可以的话会有什么弊端呢？</strong></p>
<p data-nodeid="1924" class="">下一讲，我将带你使用 U-Net 训练一个人像分割模型。</p>
<p data-nodeid="8250" class="te-preview-highlight"><img src="https://s0.lgstatic.com/i/image/M00/8B/C5/Ciqc1F_gWQeAVblhAAUYUTZTR6M316.png" alt="金句18.png" data-nodeid="8253"></p>

---

### 精选评论


