<p data-nodeid="1609" class="">06 讲我们详细讨论了写缓存的架构解决方案，它虽然可以减少数据库写操作的压力，但也存在不足。比如需要长期高频插入数据时，这个解决方案就无法满足，07 讲我们就围绕这个问题逐步提出解决方案。在架构方案层层展开的过程中，你会发现不断有新的问题需要讨论。</p>
<h3 data-nodeid="1610">业务背景（架构经历六）</h3>
<p data-nodeid="1611">因业务快速发展，某天我们公司的日活用户高达 500 万，基于现有业务模式，业务侧要求我们根据用户的行为做埋点，旨在记录用户在特定页面的所有行为、开展数据分析与第三方进行费用结算。（为什么需要费用结算，这里就不展开了。）</p>
<p data-nodeid="1612">当然，在数据埋点的过程中，业务侧还要求在后台能实时查询用户行为数据及统计报表。<strong data-nodeid="1801">（这里虽说是实时，其实特定时间内的延迟业务方还是能接受的，为确保描述准确性，我们把它称之为准实时吧。）</strong></p>
<p data-nodeid="1613">为了让你更加容易理解后续方案的设计思路，我把真实业务场景中的数据结构进行了相关简化（真实的业务场景数据结构更加复杂）。首先，我们需要收集的原始数据结构如下表所示：</p>
<p data-nodeid="2010" class=""><img src="https://s0.lgstatic.com/i/image/M00/8C/4F/CgqCHl_qkYSAVHXQAABuTN_cOVE810.png" alt="image.png" data-nodeid="2013"></p>

<p data-nodeid="1663">通过以上数据结构，在后台查询原始数据时，业务侧不仅可以以城市（根据经纬度换算）、性别（需要从业务表中抽取）、年龄（需要从业务表抽取）、目标类型、目标 ID、事件动作等作为查询条件实时查看用户行为数据，还可以以时间（天/周/月/年）、性别、年龄等维度实时查看每个目标 ID 的总点击数、平均点击次数、每个页面的转化率等统计报表数据。（当然，关于统计的需求还很多，这里我们只是列举了一小部分）。</p>
<p data-nodeid="1664">为了实现费用结算这个需求，我们需要收集的数据结构如下表所示（再次强调，该数据结构只是示例，并非真实的业务场景）：</p>
<p data-nodeid="2660" class="te-preview-highlight"><img src="https://s0.lgstatic.com/i/image/M00/8C/44/Ciqc1F_qkZmARd8uAABIaLPVH8c955.png" alt="image (1).png" data-nodeid="2667"></p>

<p data-nodeid="1690">希望以上数据结构可以帮助我们快速理解业务场景。为了快速实现整个解决方案，接下来我们探讨下技术选型的初步思路。</p>
<h3 data-nodeid="1691">技术选型思路</h3>
<p data-nodeid="1692">根据以上业务场景，我们提炼出了 6 点业务需求，并针对业务需求梳理了技术选型相关思路。</p>
<ol data-nodeid="1693">
<li data-nodeid="1694">
<p data-nodeid="1695"><strong data-nodeid="1863">原始数据海量：</strong> 对于这点，我们初步考虑使用 HBase 进行持久化。</p>
</li>
<li data-nodeid="1696">
<p data-nodeid="1697"><strong data-nodeid="1868">对于埋点记录的请求响应要快：</strong> 埋点记录服务会把原始埋点记录存放在一个缓冲的地方，以此保证响应快速。关于这点有好几个缓存方案，一会儿我们展开讨论。</p>
</li>
<li data-nodeid="1698">
<p data-nodeid="1699"><strong data-nodeid="1873">可通过后台查询原始数据：</strong> 如果使用 HBase 直接作为查询引擎，查询速度太慢了，所以我们还需要使用 ES 来保存查询页面上作为查询条件的字段和活动 id。</p>
</li>
<li data-nodeid="1700">
<p data-nodeid="1701"><strong data-nodeid="1878">各种统计报表的需求：</strong> 关于数据可视化工具也有很多选择，比如 Kibana、Grafana 等，考虑使用过程的灵活性，我们最终选择自己设计功能。</p>
</li>
<li data-nodeid="1702">
<p data-nodeid="1703"><strong data-nodeid="1883">能根据埋点日志生成费用结算数据：</strong> 我们将费用结算数据保存在 MySQL 中。</p>
</li>
<li data-nodeid="1704">
<p data-nodeid="1705"><strong data-nodeid="1888">需要一个框架将缓存中的数据进行处理，并保存到 ES、HBase 和 MySQL 中。</strong> 因为业务有准实时查询的需求，所以我们需要使用实时处理工具。目前，市面上流行的实时处理工具主要分为 Storm、Spark Streaming、Apache Flink 这三种，一会儿我们也会展开说明。</p>
</li>
</ol>
<p data-nodeid="1706">为了帮助你快速理解这部分知识，我简单画了一张架构图来说明，如下图所示：</p>
<p data-nodeid="1707"><img src="https://s0.lgstatic.com/i/image2/M01/03/E1/CgpVE1_kB8OAIjnhAABhMMWGHR4615.png" alt="Drawing 0.png" data-nodeid="1892"></p>
<p data-nodeid="1708">仔细观察这张架构图，你会发现图上还有 2 个地方打了问号，这是为什么呢？这就涉及我们接下来需要讨论的 4 个问题。</p>
<h4 data-nodeid="1709">1.使用什么技术保存埋点数据的第一现场？</h4>
<p data-nodeid="1710">市面上关于快速保存埋点数据的技术主要分为 Redis、Kafka、本地日志这三种，在上面的业务场景中，我们最终选择了本地日志。</p>
<p data-nodeid="1711">说到这，你可能想问：Redis 跟 Kafka 到底哪里不好，为什么你没使用呢？<strong data-nodeid="1901">我们先来说说 Redis 的 AOF 机制</strong>，这点在 06 讲我们也有说过。</p>
<p data-nodeid="1712">Redis 的 AOF 机制会持久化保存 Redis 所有的操作记录，用于服务器宕机后数据还原。那 Redis 什么时候将 AOF 落盘呢？</p>
<p data-nodeid="1713">在 Redis 中存在一个 AOF 配置：appendfsync，如果 appendfsync 配置成 everysec，AOF 每秒落盘一次，不过这种配置方式有可能会丢失 1 秒的数据。如果 appendfsync 配置成 always，每次操作请求的记录都落盘后再返回成功信息给客户端，不过这种配置方式系统性能就会很慢。因为对于埋点记录的请求要求响应快，所以我们没有选择 Redis。</p>
<p data-nodeid="1714">接下来我们讨论下<strong data-nodeid="1909">Kafka 的技术方案</strong>。</p>
<p data-nodeid="1715">Kafka 的冗余设计是每个分区都有多个副本，其中一个副本是 Leader，其他副本都是 Follower，Leader 主要负责处理所有的读写请求，并同步数据给其他 Follower。</p>
<p data-nodeid="1716">那么 Kafka 什么时候将数据从 Leader 同步给 Follower 呢 ？Kafka 的 producer configs 中也有个 acks 配置，它的配置方式分为三种。</p>
<ol data-nodeid="1717">
<li data-nodeid="1718">
<p data-nodeid="1719">acks=0：不等 Leader 将数据落到日志，Kafka 直接返回完成信号给客户端。这种方式虽然响应快，但数据持久化没有保障，数据如果没有落到本地日志，系统就会出现宕机，导致数据丢失。</p>
</li>
<li data-nodeid="1720">
<p data-nodeid="1721">acks=1：等 Leader 将数据落到本地日志，但是不等 Follower 同步数据，Kafka 就直接返回完成信号给客户端。</p>
</li>
<li data-nodeid="1722">
<p data-nodeid="1723">acks=all：等 Leader 将数据落到日志，且等 <a href="https://kafka.apache.org/documentation.html" data-nodeid="1917">min.insync.replicas</a> 个 Follower 都同步数据后，Kafka 再返回完成信号给客户端。这种配置方式虽然数据有保证，但响应慢。</p>
</li>
</ol>
<p data-nodeid="1724">通过以上内容的讲解，我们发现使用 Redis 与 Kafka 都会出现问题。</p>
<p data-nodeid="1725">如果我们想保证数据的可靠性，必然需要牺牲系统性能，那有没有一个方案可以性能+可靠性同时兼得呢？有的，所以我们最终决定把埋点数据保存到本地日志中。</p>
<h4 data-nodeid="1726">2.使用什么技术（ES、HBase、MySQL）把缓冲的数据搬到持久化层？</h4>
<p data-nodeid="1727">关于这个问题，最简单的方式是通过 Logstash 直接把日志文件中的数据搬运到 ES，但是问题来了，业务侧要求存放 ES 中的记录包含城市、性别、年龄等原始数据（这些字段需要调用业务系统的数据进行抽取），而这些原始数据日志文件中并没有，所以我们并没有选择 Logstash。</p>
<p data-nodeid="1728">如果你坚持通过 Logstash 把日志文件的数据搬运到 ES，我分享 3 种实现方式。</p>
<ol data-nodeid="1729">
<li data-nodeid="1730">
<p data-nodeid="1731"><strong data-nodeid="1928">自定义 filter：</strong> 先在 Logstash 自定义的 filter 里封装业务数据，再保存到 ES。因 Logstash 自定义的 filter 是使用 Ruby 语言编写的，也就是说我们需要使用其他语言编写业务逻辑，因此 Logstash 自定义 filter 的方案被我们 pass 了。</p>
</li>
<li data-nodeid="1732">
<p data-nodeid="1733"><strong data-nodeid="1933">修改客户端的埋点逻辑：</strong> 每次记录埋点的数据发送到服务端之前，我们先在客户端将业务的相关字段提取出来再上传到服务端。这个方法也直接被业务端 pass 了，理由是后期业务侧每更新一次后台查询条件，我们就需要重新发一次版，实在太麻烦了。</p>
</li>
<li data-nodeid="1734">
<p data-nodeid="1735"><strong data-nodeid="1938">修改埋点服务端的逻辑：</strong> 每次服务端在记录埋点的数据发送到日志文件之前，我们先从数据库获取业务字段组合埋点记录。这个方法也被服务端 pass 了，因为这种操作会直接影响每个请求的效率，间接影响用户体验。</p>
</li>
</ol>
<p data-nodeid="1736">另外，我们没选择 Logstash 还有 2 点原因。</p>
<ul data-nodeid="1737">
<li data-nodeid="1738">
<p data-nodeid="1739">日志文件中的数据需要同时输出 ES 和 Hbase 两个输出源，因 Logstash 的多输出源基于同一个 pipeline，如果 1 个输出源出错了，另 1 个输出源也会出错，两者之间会互相影响。</p>
</li>
<li data-nodeid="1740">
<p data-nodeid="1741">MySQL 中需要生成费用结算数据，而费用结算数据需要通过分析埋点的数据动态来计算，显然 Logstash 并不适合这样的业务场景，因为 filter 可以改变每条数据某些字段的值。</p>
</li>
</ul>
<p data-nodeid="1742">在上面的业务场景中，我们最终决定引入了一个计算框架了，此时整个解决方案的架构图如下：</p>
<p data-nodeid="1743"><img src="https://s0.lgstatic.com/i/image2/M01/03/E0/Cip5yF_kB-2AFFKxAABmCBVQSx8720.png" alt="Drawing 1.png" data-nodeid="1945"></p>
<p data-nodeid="1744">这个方案中就是先通过 Logstash 把日志文件搬运到 MQ 中，再通过实时计算框架处理 MQ 中的数据，最后保存处理转换出来的数据到持久层中。</p>
<p data-nodeid="1745"><strong data-nodeid="1950">实际上，引入实时计算框架是为了在原始的埋点数据中填充业务数据，并统计埋点数据生成费用结算数据，最后分别保存到持久层中。</strong></p>
<p data-nodeid="1746">最后，关于 Logstash 的注意点，我们需要重点强调下。</p>
<p data-nodeid="1747">Logstash 系统是通过 Ruby 语言编写的，资源消耗大，所以官方又推出了一个轻量的 Filebeat。我们可以使用 Filebeat 收集数据，再通过 Logstash 进行数据过滤。如果你不想使用 Logstash 的强大过滤功能，你可以直接使用 Filebeat 收集日志数据发送给 Kafka。</p>
<p data-nodeid="1748">但问题又来了，Filebeat 是使用轮询的方式采集文件变动，存在一定（有时候很大）延时，不像 Logstash 可直接监听文件变动，所以最终我们选择继续使用 Logstash。（因为我们扛得住资源的消耗。）</p>
<p data-nodeid="1749">讨论到这，上图中绿色部分的内容已确认，接下来我们开始讨论橙色部分的内容：Kafka、处理框架。</p>
<h4 data-nodeid="1750">3.为什么使用 Kafka？</h4>
<p data-nodeid="1751">Kafka 是 LinkedIn 推出的开源消息中间件，它天生是为收集日志而设计，且它具备超高的吞吐量和数据量的扩展性，号称无限堆积。</p>
<p data-nodeid="1752">根据LinkedIn官方说法，他们使用 3 台便宜的机器部署 Kafka，就能每秒写入 2 百万条记录，官方博客截图如下图所示：</p>
<p data-nodeid="1753"><img src="https://s0.lgstatic.com/i/image2/M01/03/E0/Cip5yF_kB_uAH-DqAABG5nHyo2Y419.png" alt="Drawing 2.png" data-nodeid="1960"></p>
<div data-nodeid="1754"><p style="text-align:center">（来源：LinkedIn官方博客）</p></div>
<p data-nodeid="1755">看到这，你肯定会好奇为什么它的吞吐量这么高？这里我们就有必要了解 Kafka 的存储结构了，我们先看一张架构示意图，如下图所示：</p>
<p data-nodeid="1756"><img src="https://s0.lgstatic.com/i/image2/M01/03/E0/Cip5yF_kCA6ABXQvAAJ-PLV0gmA327.png" alt="Drawing 3.png" data-nodeid="1964"></p>
<blockquote data-nodeid="1757">
<p data-nodeid="1758">图片来源 ：Kafka 官方文档：http://kafka.apache.org/documentation/#log</p>
</blockquote>
<p data-nodeid="1759">Kafka 的存储结构中每个 Topic 分区相当于 1 个巨型文件，而每个巨型文件又是由多个 segment 小文件组成。其中，Producer 负责对该巨型文件进行“顺序写”，Consumer 负责对该文件进行“顺序读”。</p>
<p data-nodeid="1760">这里，我们可以把 Kafka 的存储架构简单理解为 Kafka 写数据通过追加数据到文件尾实现顺序写，读取数据时直接从文件中读，好处是读操作不会阻塞写操作，这也是吞吐量大的原因。</p>
<p data-nodeid="1761">另外，理论上只要磁盘空间足够，Kafka 可以实现消息无限堆积，因此它特别适合处理日志收集这种场景，可见我们选择使用 Kafka 是有一定理论依据的哦。</p>
<h4 data-nodeid="1762">4.使用什么技术把 Kafka 的数据搬运到持久化层？</h4>
<p data-nodeid="1763">为了把 Kafka 的数据搬运到持久层，我们需要使用一个分布式实时计算框架，原因有 2 点。</p>
<ol data-nodeid="1764">
<li data-nodeid="1765">
<p data-nodeid="1766">数据量特别大，为此我们需要使用一个处理框架将上亿的埋点数据每天进行快速分析和处理（且必须使用多个节点并发处理才来得及），再存放到 ES、HBase 和 MySQL 中，即大数据计算，因此它有分布式计算的诉求。</p>
</li>
<li data-nodeid="1767">
<p data-nodeid="1768">业务要求实时查询统计报表数据，因此我们需要一个实时计算框架处理埋点数据。</p>
</li>
</ol>
<p data-nodeid="1769">目前，市面上流行的分布式实时计算框架有 3 种：Storm、Spark Stream、Apache Flink，到底使用哪个好呢？</p>
<p data-nodeid="1770">我认为都可以，这就看公司的具体情况了，比如公司已经使用实时计算框架了，你就不需要再考虑这个问题了，如果公司还没有使用，那就看个人喜好了。</p>
<p data-nodeid="1771">我个人偏好 Apache Flink，不仅因为它性能强（听说阿里双 11 使用它后，1 秒内处理了 17 亿条数据），还因为它的容错机制能保证每条数据仅仅处理 1 次，且它有时间窗口处理功能。</p>
<p data-nodeid="1772">关于流处理、容错机制、时间窗口这三个概念，我们具体展开说明一下。</p>
<p data-nodeid="1773">在流处理这个过程中，往往会引发一系列的问题，比如一条消息处理过程中，如果系统出现故障该怎么办？你会重试吗？如果重试会不会出现重复处理？如果不重试，消息是否会丢失？你能保证每条消息最多或最少处理几次？</p>
<p data-nodeid="1774">在不同流处理框架中采取不同的容错机制，它们也就保证了不一样的一致性。</p>
<ol data-nodeid="1775">
<li data-nodeid="1776">
<p data-nodeid="1777"><strong data-nodeid="1983">At-Most-Once :</strong> 至多一次，表示一条消息不管后续处理成功与否只会被消费处理一次，存在数据丢失可能。</p>
</li>
<li data-nodeid="1778">
<p data-nodeid="1779"><strong data-nodeid="1988">Exactly-Once :</strong> 精确一次，表示一条消息从其消费到后续的处理成功，只会发生一次。</p>
</li>
<li data-nodeid="1780">
<p data-nodeid="1781"><strong data-nodeid="1993">At-Least-Once ：</strong> 至少一次，表示一条消息从消费到后续的处理成功，可能会发生多次，存在重复消费的可能。</p>
</li>
</ol>
<p data-nodeid="1782">以上三种方式中，Exactly-Once 无疑是最优的选择，因为在正常的业务场景中，一般只要求消息处理一次。而 Apache Flink 的容错机制就可以保证所有消息只处理 1 次（Exactly-Once）的一致性，还能保证系统安全性能，所以很多人最终都使用它。</p>
<p data-nodeid="1783">接下来，我们来说说 Apache Flink 的时间窗口计算功能，以下是 Apache Flink 的一个代码示例，它把每个小时里发生事件的用户聚合在一个列表中。</p>
<pre class="lang-java" data-nodeid="1784"><code data-language="java"><span class="hljs-keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);
<span class="hljs-comment">// alternatively:</span>
<span class="hljs-comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span>
<span class="hljs-comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span>
DataStream&lt;MyEvent&gt; stream = env.addSource(<span class="hljs-keyword">new</span> FlinkKafkaConsumer09&lt;MyEvent&gt;(topic, schema, props));
stream
    .keyBy( (event) -&gt; event.getUser() )
    .timeWindow(Time.hours(<span class="hljs-number">1</span>))
    .reduce( (a, b) -&gt; a.add(b) )
    .addSink(...);
</code></pre>
<p data-nodeid="1785">我们知道，日志中事件发生的时间有可能与计算框架处理消息的时间不一致。</p>
<p data-nodeid="1786">假定实时计算框架收到消息的时间是 2 秒后，比如有一条消息，这个事件发生的时间是 6:30，因你接收到消息后处理的时间延后了 2 秒，即变成了 6:32，因此当你计算 6:01-6:30 的数据和，这条消息并不会计算在 6:01-6:30 范围内，这就不符合实际的业务需求了。</p>
<p data-nodeid="1787"><strong data-nodeid="2001">在实际业务场景中，如果需要按照时间窗口统计数据，我们往往是根据消息的事件时间来计算。而 Apache Flink 的特性恰恰是基于消息的事件时间，而不是基于计算框架的处理时间，这也是它的另一个撒手锏。</strong></p>
<p data-nodeid="1788">为了方便你理解，我也画了一张图来说明，此时整个架构设计方案如下图所示：</p>
<p data-nodeid="1789"><img src="https://s0.lgstatic.com/i/image2/M01/03/E1/CgpVE1_kCDKAWxpVAABk7VDnTI4369.png" alt="Drawing 4.png" data-nodeid="2005"></p>
<h3 data-nodeid="1790">总结</h3>
<p data-nodeid="1791">07 讲中，我们并没有讲解一些特别深入的架构设计上的注意点，主要是阐述技术选型背后的思考过程，希望对你的架构思维的提升有所帮助。</p>
<p data-nodeid="1792">07 讲中探讨的解决方案肯定还存在遗漏的问题没有考虑到，如果你有更好的方案，欢迎在评论区留言，对本专栏感兴趣的同学欢迎分享给更多的好友看到哦。</p>
<p data-nodeid="1793" class="">08 讲我们开始讲解讲秒杀架构，秒杀架构是一个综合性非常强的问题，并且在面试时经常会被问到，所以我强烈建议你多关注一下。</p>

---

### 精选评论

##### 陈：
> 老师你好，本地日志文件具体是怎么写入， 是用log4j2 直接写入到单独的一个文件吗

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 嗯，就是这样。

##### **—X厂—劝退师：
> 可不可以收集用户数据 后，直接异步存储到ES中，采用ES聚合统计分析的功能，实现业务需求

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 异步到ES之前需要一个缓存区，这个缓存区不建议直接用jvm的内存，会丢数据。

##### *懂：
> 时间问题 是不是应该客户端只发送utc时间 日志保存utc时间 取的时候在转换

 ###### &nbsp;&nbsp;&nbsp; 讲师回复：
> &nbsp;&nbsp;&nbsp; 我做的时候没有跨时区的问题，如果有跨时区的问题是这样的。

