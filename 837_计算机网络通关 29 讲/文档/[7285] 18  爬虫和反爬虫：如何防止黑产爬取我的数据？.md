<p data-nodeid="481" class="">爬虫在当今的互联网中被大量地使用已经是约定俗成的潜规则，虽说内容的提供者都千方百计地防止自己的数据被竞品拿走，但是如果你去看一看某些百科中的文章和维基百科的相似程度，就知道很多不良的行为正在被默许着。</p>
<p data-nodeid="482">记得早期一些购票网站起家的时候，就大量使用爬虫技术爬取航空公司的数据，为了不让航空公司屏蔽，特意用了很多个人电脑做爬虫端，让航空公司无法分清哪些是爬虫、哪些是用户。这样，用户订票的时候，客服经理就有足够的票务数据提供给用户。</p>
<p data-nodeid="483">另外，一些相互竞争的电商、外卖公司，内部甚至会设立专门的数据爬取小组，用于监控竞品的数据，并且实时地调整业务的竞争策略——如补贴、签约等。还有一些爬虫的黑产利用招聘网站的漏洞，爬取并出售简历数据。对一个 HR 而言，花几千块钱的年费，才可以看上万份简历。而一个黑产，只需要多购买几个这样的账号，就可以从招聘网站中拿走大量的数据，再销售给不法分子，一年获得上千万的利润。</p>
<p data-nodeid="484">因此，这一讲我们就聊一聊这个的话题——<strong data-nodeid="538">如何防止黑产爬取我的数据</strong>，以此加深你对数据安全的重视。</p>
<h3 data-nodeid="485">爬取数据违法吗？</h3>
<p data-nodeid="486">首先，爬取一个网站的数据，很可能是违法行为。通常一个网站，会在自己根路径下的 robots.txt 中定义自己网页中哪些数据是可以用来爬取的。从理论上讲，如果你想爬取一个网站的数据，应该先获取它根目录下的 robots.txt 文件，查阅文件内容，看自己要爬取的数据是否被允许。</p>
<p data-nodeid="487">下面是 bilibili 的 robots.txt 的内容：</p>
<pre class="lang-java" data-nodeid="488"><code data-language="java">User-agent: Yisouspider
Allow: /
User-agent: Applebot
Allow: /
User-agent: bingbot
Allow: /
User-agent: Sogou inst spider
Allow: /
User-agent: Sogou web spider
Allow: /
User-agent: <span class="hljs-number">360</span>Spider
Allow: /
User-agent: Googlebot
Allow: /
User-agent: Baiduspider
Allow: /
User-agent: Bytespider
Allow: /
User-agent: PetalBot
Allow: /
User-agent: *
Disallow: /
</code></pre>
<p data-nodeid="489">可以看到，如果你是谷歌、苹果、360、百度等搜索引擎，那么 B 站是欢迎你爬取内容的。如果你是其他的个人或者组织，比如说你想爬取 B 站上所有大 V 的数据，然后将分析结果出售给其他人（比如某个 MCN 平台），实际上是触犯法律的。依据我国的刑法，你可能会被判处非法获取计算机信息系统数据罪，情节严重的可能会被判处 3 年以上的有期徒刑并处罚金。</p>
<p data-nodeid="490">我之所以说这件事情，是希望你对网络信息安全有清晰的认识。互联网不是法外之地，做任何事情之前都请三思而行。</p>
<h4 data-nodeid="491">助点机器人</h4>
<p data-nodeid="492">在没有允许的情况下爬取对方的数据是违法行为。但是这里衍生出一个问题，比如说，你是一个拉勾的付费用户，你觉得拉勾的界面不够智能，于是你自己写了一个程序，只针对自己的账号范围实现某个功能，对拉勾的简历进行筛选，从而找到合适的求职者，这是违法行为吗？</p>
<p data-nodeid="493">这个行为不是违法行为。这个行为可以归结成你自己做的一个辅助自己工作的机器人，但是如果你将这个工具提供给其他人，这是违法行为吗？其实也不是违法行为。但是如果其他人将这个工具用作黑产，比如说爬取用户的数据然后进行简历信息的买卖，这就构成了违法行为，构成犯罪的是买卖简历信息。如果你是拉勾的竞品，你使用大量账号这样做，还会构成非法竞争。</p>
<p data-nodeid="494">换一个例子，有人觉得 Github 不够智能，然后做了一个插件，帮助大家浏览 Github 中文件代码的目录树，本质上这个工具也需要用到爬虫的部分技术——需要爬取这个目录树。但这不是违法行为，但若有人利用类似的工具，将 Github 全部代码都拿走，在淘宝上打包售卖，这就是违法行为了。</p>
<h3 data-nodeid="495">爬虫的原理</h3>
<p data-nodeid="496">讨论完法律，我们讨论下爬虫是怎么实现的。爬虫的原理非常简单，本质上就是一次网络请求，然后将返回的数据保存下来。</p>
<p data-nodeid="497">对于搜索引擎的爬虫而言，通常会在请求头中加上自己的标识，比如百度会加上 baidu 字符串，这样方便网站服务器识别。</p>
<p data-nodeid="1327" class="">爬虫如果是非法的，往往就需要伪装成浏览器。通常会用到浏览器内核去模拟发出网络请求，比如用 Chromium（Chrome 的开源内核）就可以提供这样的能力。</p>




<p data-nodeid="2295" class="">当你用 Chromium 发起请求的时候，对于服务提供方的反爬虫系统，你的请求就变成了一次标准的用户行为。如果对方网站需要登录才能爬取数据，这个时候，不法分子还会模拟登陆行为。如果仅仅是输入用户名和密码，那这个网站登录行为会非常容易模拟，只需要找到对方对应的接口，把用户名和密码传过去，就可以拿到访问资源的令牌。这就是大部分网站登录时需要你用手机验证码登录、微信扫描、或填写图片验证码的原因。</p>




<p data-nodeid="500">对于一些获取数据还需要付费的网站，比如说视频网站或拉勾这样的招聘网站，用户需要付费才能获取核心的数据，这个时候不法分子可能会购买大量的账号。为了防止不法分子获得大量的账号，现在国家已经在严打销售手机卡号的行为。所以请你记住，使用其他人的身份去注册账号，这也是一种违法行为。</p>
<h4 data-nodeid="501">关于验证码</h4>
<p data-nodeid="502">当被爬取的网站登录接口有验证码时，爬虫的设计者通常会有两种手段。一种是破解验证码，在现在这个人工智能的时代，想要破解验证码只需要获得足够多的验证码图片样本，然后用 tensorflow 分析一下，基本上都可以做到一定的识别率，可以高于 80% 以上。所以现在的网站往往不会使用简单的图片验证码，比如说要拖动一个滑块、选中几张图片、算一道数学题等来增加破解成本。我见过最变态的网站验证码是一道化学题，我花了两个小时才注册成功。</p>
<p data-nodeid="503">所以你的网站如果还在使用普通的图形验证码，而你网站被攻克的代价也很高的话，请你务必早点更换验证码——更换成更难破解的，甚至多种验证码的混合。</p>
<h4 data-nodeid="504">模拟用户动作</h4>
<p data-nodeid="505">对于一个爬取数据用的浏览器内核，往往还提供了模拟用户行为的功能。比如说点击按钮，滚动一下页面，输入一行文字。所以千万不要觉得，爬虫模拟不了这些用户行为，对于爬虫的设计者，这些都是基础操作。</p>
<h4 data-nodeid="506">数据的提取</h4>
<p data-nodeid="507">当数据被下载下来之后，爬虫会尝试将原始数据存储，然后再进行离线分析。当然有的爬虫爬取了数据之后就马上进行分析。如果要爬取网页数据，后续会用到 HTML 的解析器（Parser），这个在 Github上 可以找到很多的开源实现。如果是爬取的接口数据，通常就是分析 Json。有的网页数据是由 JavaScript 渲染的，这种网页，通常爬虫会模拟浏览器的行为，在页面加载完成几秒之后才开始下载网页内容。</p>
<h4 data-nodeid="508">反追踪</h4>
<p data-nodeid="509">对于黑产的爬虫，还会进行 IP 的反追踪。所谓 IP 的反追踪，就是利用代理，增加追踪的成本。比如黑客在从事犯罪活动时通过多次代理，跨了多个国家，那么一个国家的警方力量就很难追踪到他。在爬虫领域有很多人会购买 IP 代理，比如说一个非法的去 B 站收集统计数据的爬虫，为了防止 B 站的追诉以及防止 B 站安全策略的屏蔽，可能会购买大量的 IP，然后模拟成几百个用户在使用 B 站。你要注意，临时租用大量 IP 地址的价格低廉，这也大大降低了犯罪的成本。</p>
<h3 data-nodeid="510">反爬虫</h3>
<p data-nodeid="511">接下来，我们说说有关反爬虫的一些基本的操作。</p>
<h4 data-nodeid="512">robots.txt</h4>
<p data-nodeid="513"><strong data-nodeid="570">在反爬虫的时候，第一步我们要先从法律上告诉爬虫哪些页面是不可以爬取的</strong>。所以我们要先写好自己的 robots.txt，并放到网站的根目录。</p>
<h4 data-nodeid="514">用户的识别</h4>
<p data-nodeid="515">接下来我们对于高频访问的 IP 要予以关注。当然，仅仅通过 IP 来判断是不可取的。因为有的时候一家公司会共用一个 IP 出口地址。举个例子：一家猎头公司下面的几百个猎头，可能会每天疯狂的使用拉勾，因此从拉勾的数据上，你会看到大量的重复 IP 访问。这个时候我问你个问题，你禁不禁用这些 IP？当然不能禁用，这些都是付费用户。</p>
<p data-nodeid="516">那么这个时候有一件非常值得做的事情，就是使用设备的指纹。对于一个设备，它的 CPU 数量、CPU 序列号、屏幕的分辨率、手机的厂商等，通常是固定的。这样可以结合 IP 地址做精细去重。这项技术被称为<strong data-nodeid="578">设备指纹</strong>，就是利用设备上的信息，生成一个具有唯一性的字符串，因为这种生成算法是非标准化的，因此不同的数据安全团队会有自己的算法。</p>
<p data-nodeid="517">有了对用户的识别，就可以根据唯一用户设置数据安全策略，比如访问频次、黑名单等。</p>
<h4 data-nodeid="518">字体加密</h4>
<p data-nodeid="519">再介绍一种方法是<strong data-nodeid="586">自己实现字符编码和字体文件，增加爬虫爬取数据的成本</strong>。</p>
<p data-nodeid="520">爬虫爬取的通常就是用户本身可以看到的内容。如果自己实现一套自己的字符编码。比如将 UTF8 编码中的汉字打乱顺序，然后再将字体文件中对应的数据换序，得到字体文件。显示简历的时候，使用自己根据这个字符集生成的字体文件。</p>
<p data-nodeid="521">这样，爬虫下载到网页数据后，中文会乱码，这是因为爬虫无法理解我们创造的非标准字符集编码。当用户看到网页的时候，可以看到正确的内容，这是因为字体文件起了作用。即便爬虫将字体文件打开，和编码对应上，也是非常复杂的一个体力劳动。然后我们每天更换一次顺序，就可以给黑产增加相当大的爬取成本。</p>
<h4 data-nodeid="522">加密传输</h4>
<p data-nodeid="523">对于移动端 App 中的数据，如果可以加密传输，也能大大增加爬取成本。因为 App 不是浏览器，想要模拟一个 App 是非常困难的。那么 App 的数据抓取就依赖于 App 数据传输使用的标准协议，比如一个用 HTTPS 协议传输数据的 App，爬虫可以在 App 端安装证书，然后再利用代理实现中间人抓包。但如果数据用自己的协议加密，那么爬虫抓包的同时，还必须能够破解这个加密协议。</p>
<h3 data-nodeid="524">总结</h3>
<p data-nodeid="525">非法爬取数据是不可能完全杜绝的，我们只能提高非法爬取数据的成本。但是一定要有数据安全的意识。在互联网的世界里，数据是第一生产力，也是生命线。在完成开发工作之余，利用自己的专业知识适当提高爬取数据的成本是非常有必要的。</p>
<p data-nodeid="526" class=""><strong data-nodeid="597">如果自己被公司要求写一个爬虫爬取竞品数据，请你先阅读下竞品的 robots.txt 文件，看看允不允许你这样做</strong>。如果这是一个违法行为，那么也可以适当提醒下有这样想法的决策者。 国家对网络信息安全犯罪的打击，只会越来越严。爬取数据看似简单，其实做到毫无证据保留是很难的。当然，利用爬虫技术，让自己在使用互联网产品的时候，可以消耗更少的时间，属于辅助机器人，这个是法律允许的。比如我就用爬虫技术监控拉勾教育中我自己专栏的订阅情况，当有同学订阅的时候我会收到邮件。</p>
<h3 data-nodeid="527">思考题</h3>
<p data-nodeid="528">最后再给你提一个问题：用最熟悉的语言写一段程序，模拟成浏览器访问拉勾教育的首页获取首页数据。</p>
<p data-nodeid="529" class="">这一讲就到这里，发现求知的乐趣，我是林䭽。感谢你学习本次课程，下一讲是《模块四思考题解答》，希望你自己完成题目后再来看答案和分析。再见！</p>

---

### 精选评论

##### **潮：
> 正在琢磨怎么穿透内网。有帮助。

